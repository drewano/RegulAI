"""
Agent LangGraph principal utilisant l'API fonctionnelle pour interagir avec le serveur MCP L√©gifrance.

Cet agent suit le patron d'architecture ReAct et utilise les d√©corateurs @task et @entrypoint
de LangGraph pour orchestrer les interactions avec le serveur MCP.

Implementation fid√®le du guide how-tos/react-agent-from-scratch-functional.ipynb
"""

import os
from typing import cast
from langchain_core.messages import ToolMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.func import entrypoint, task
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from .tools import get_available_tools


def _set_env(var: str):
    """Helper pour d√©finir une variable d'environnement si elle n'existe pas."""
    if not os.environ.get(var):
        import getpass
        os.environ[var] = getpass.getpass(f"{var}: ")


# Configuration du mod√®le LLM
_set_env("OPENAI_API_KEY")
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# R√©cup√©ration des outils MCP disponibles
tools = get_available_tools()
tools_by_name = {tool.name: tool for tool in tools}

# Configuration du checkpointer pour la persistance
checkpointer = MemorySaver()


@task
def call_model(messages):
    """Call model with a sequence of messages."""
    response = model.bind_tools(tools).invoke(messages)
    return response


@task
def call_tool(tool_call):
    """Execute tool call and return result as ToolMessage."""
    tool = tools_by_name[tool_call["name"]]
    observation = tool.invoke(tool_call["args"])
    return ToolMessage(content=str(observation), tool_call_id=tool_call["id"])


@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    """
    Agent principal utilisant l'architecture ReAct.
    
    Implementation fid√®le de l'exemple react-agent-from-scratch-functional.ipynb
    avec persistance des conversations.
    """
    # Ajouter les messages pr√©c√©dents si ils existent (persistance)
    if previous is not None:
        messages = add_messages(previous, messages)
    
    # Appel initial du mod√®le
    llm_response = cast(AIMessage, call_model(messages).result())
    
    # Boucle ReAct: raisonnement et action
    while True:
        # Si pas d'appels d'outils, on termine
        if not llm_response.tool_calls:
            break
        
        # Ex√©cuter les outils en parall√®le
        tool_result_futures = [
            call_tool(tool_call) for tool_call in llm_response.tool_calls
        ]
        tool_results = [fut.result() for fut in tool_result_futures]
        
        # Ajouter la r√©ponse du LLM et les r√©sultats des outils aux messages
        messages = add_messages(messages, [llm_response, *tool_results])
        
        # Appeler le mod√®le √† nouveau avec les nouveaux r√©sultats
        llm_response = cast(AIMessage, call_model(messages).result())
    
    # G√©n√©rer la r√©ponse finale et sauvegarder l'√©tat
    messages = add_messages(messages, llm_response)
    return entrypoint.final(value=llm_response, save=messages)


def main():
    """Fonction principale pour tester l'agent."""
    # Message de test
    user_message = HumanMessage(content="Aide-moi √† chercher des informations sur le droit du travail fran√ßais.")
    
    print("ü§ñ Agent L√©gifrance d√©marr√©!")
    print(f"User: {user_message.content}")
    print("\n--- R√©ponse de l'agent ---")
    
    # Configuration pour la persistance
    config: RunnableConfig = {"configurable": {"thread_id": "test-thread"}}
    
    # Streaming de la r√©ponse
    for step in agent.stream([user_message], config=config):
        for task_name, message in step.items():
            if task_name == "agent":
                continue  # On affiche seulement les mises √† jour des t√¢ches
            print(f"\n{task_name}:")
            if hasattr(message, 'pretty_print'):
                message.pretty_print()
            else:
                print(message)


if __name__ == "__main__":
    main() 