"""
Agent RegulAI principal avec architecture StateGraph.

Ce module assemble les n≈ìuds d√©finis dans graph.py pour cr√©er l'agent LangGraph
complet avec persistance et streaming.
"""

from typing import Optional, Any, Dict
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.graph import CompiledGraph

from .config import get_config
from .graph import AgentState, call_model, call_tools, should_continue


def create_agent(checkpointer: Optional[MemorySaver] = None) -> CompiledGraph:
    """
    Cr√©e et compile l'agent RegulAI avec architecture StateGraph.
    
    Args:
        checkpointer: Checkpointer pour la persistance (utilise MemorySaver par d√©faut)
        
    Returns:
        Agent LangGraph compil√© et pr√™t √† utiliser
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    # Valider la configuration
    config = get_config()
    
    # Cr√©er le StateGraph avec l'√©tat AgentState
    workflow = StateGraph(AgentState)
    
    # Ajouter les n≈ìuds
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", call_tools)
    
    # D√©finir le point d'entr√©e
    workflow.set_entry_point("agent")
    
    # Ajouter les ar√™tes conditionnelles
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "__end__": END,
        },
    )
    
    # Apr√®s les outils, retourner √† l'agent
    workflow.add_edge("tools", "agent")
    
    # Compiler le graphe avec un checkpointer
    if checkpointer is None:
        checkpointer = MemorySaver()
    
    compiled_graph = workflow.compile(checkpointer=checkpointer)
    
    return compiled_graph


def run_agent_conversation(
    message: str, 
    thread_id: str = "default-session",
    agent: Optional[CompiledGraph] = None
) -> str:
    """
    Lance une conversation avec l'agent RegulAI.
    
    Args:
        message: Message de l'utilisateur
        thread_id: ID de session pour la persistance
        agent: Instance de l'agent (cr√©√©e automatiquement si None)
        
    Returns:
        R√©ponse finale de l'agent
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    if agent is None:
        agent = create_agent()
    
    # Cr√©er le message utilisateur
    user_message = HumanMessage(content=message)
    
    # Configuration pour la persistance
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
    
    # Invoquer l'agent
    result = agent.invoke(
        {"messages": [user_message]},
        config=config
    )
    
    # R√©cup√©rer la r√©ponse finale
    if result and "messages" in result:
        last_message = result["messages"][-1]
        if hasattr(last_message, 'content'):
            return last_message.content
    
    return "Aucune r√©ponse de l'agent"


def stream_agent_conversation(
    message: str,
    thread_id: str = "default-session", 
    agent: Optional[CompiledGraph] = None
):
    """
    Lance une conversation avec streaming avec l'agent RegulAI.
    
    Args:
        message: Message de l'utilisateur
        thread_id: ID de session pour la persistance
        agent: Instance de l'agent (cr√©√©e automatiquement si None)
        
    Yields:
        √âtapes interm√©diaires et r√©ponse finale de l'agent
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    if agent is None:
        agent = create_agent()
    
    # Cr√©er le message utilisateur
    user_message = HumanMessage(content=message)
    
    # Configuration pour la persistance
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
    
    # Streamer les √©tapes avec mode "updates" pour capturer les appels d'outils
    # et les transitions entre n≈ìuds
    for step in agent.stream(
        {"messages": [user_message]},
        config=config,
        stream_mode="updates"
    ):
        yield step


def stream_agent_conversation_with_tokens(
    message: str,
    thread_id: str = "default-session", 
    agent: Optional[CompiledGraph] = None
):
    """
    Lance une conversation avec streaming token par token avec l'agent RegulAI.
    
    Cette fonction utilise le mode "messages" pour obtenir les tokens individuels
    du mod√®le de langage, permettant un affichage progressif plus granulaire.
    
    Args:
        message: Message de l'utilisateur
        thread_id: ID de session pour la persistance
        agent: Instance de l'agent (cr√©√©e automatiquement si None)
        
    Yields:
        Tokens et m√©tadonn√©es de streaming
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    if agent is None:
        agent = create_agent()
    
    # Cr√©er le message utilisateur
    user_message = HumanMessage(content=message)
    
    # Configuration pour la persistance
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
    
    # Streamer avec mode "messages" pour obtenir les tokens individuels
    for token, metadata in agent.stream(
        {"messages": [user_message]},
        config=config,
        stream_mode="messages"
    ):
        yield token, metadata


def main():
    """
    Fonction principale pour tester l'agent RegulAI.
    
    Lance une conversation de test avec l'agent et affiche les r√©sultats.
    """
    try:
        print("ü§ñ Agent RegulAI - Test de d√©marrage")
        print("=" * 50)
        
        # V√©rifier la configuration
        config = get_config()
        if not config.google_api_key:
            print("‚ùå Erreur: GOOGLE_API_KEY non configur√©")
            print("Copiez .env.example vers .env et remplissez votre cl√© API Google")
            return
        
        print(f"‚úÖ Configuration valid√©e")
        print(f"   - Mod√®le: {config.model_name}")
        print(f"   - Serveur MCP: {config.mcp_server_url}")
        print(f"   - Temperature: {config.model_temperature}")
        
        # Cr√©er l'agent
        print("\nüìù Cr√©ation de l'agent...")
        agent = create_agent()
        print("‚úÖ Agent cr√©√© avec succ√®s")
        
        # Message de test
        test_message = "Bonjour ! Peux-tu m'aider √† rechercher des informations sur les cong√©s pay√©s en France ?"
        print(f"\nüë§ Utilisateur: {test_message}")
        print("\nü§ñ Agent RegulAI:")
        print("-" * 30)
        
        # Lancer la conversation avec streaming
        for step in stream_agent_conversation(test_message, "test-session", agent):
            if "messages" in step:
                last_message = step["messages"][-1]
                if hasattr(last_message, 'content') and last_message.content:
                    # Afficher seulement les nouveaux contenus (pas les appels d'outils)
                    if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
                        print(f"üí¨ {last_message.content}")
        
        print("\n" + "=" * 50)
        print("‚úÖ Test termin√© avec succ√®s !")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors du test: {e}")
        print("\nV√©rifiez votre configuration :")
        print("1. GOOGLE_API_KEY est d√©fini dans .env")
        print("2. Le serveur MCP est d√©marr√© (optionnel pour ce test)")


if __name__ == "__main__":
    main() 