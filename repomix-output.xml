This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: openapi.json
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.env.example
.gitignore
.repomixignore
examples/__init__.py
examples/simple_conversation.py
langgraph.json
PROJET_STRUCTURE.md
pyproject.toml
scripts/__init__.py
scripts/validate_tools.py
services/__init__.py
services/mcp/__init__.py
services/mcp/main.py
services/mcp/utils.py
src/regulai/__init__.py
src/regulai/agent.py
src/regulai/config.py
src/regulai/graph.py
src/regulai/tools.py
streamlit_app.py
test_app_final.py
test_streaming_mock.py
test_streaming.py
tests/__init__.py
tests/conftest.py
tests/test_tools.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".env.example">
# Configuration RegulAI
OPENAI_API_KEY=your_key_here
MCP_SERVER_URL=http://localhost:8000
MCP_TIMEOUT=30
MODEL_NAME=gpt-4o-mini
MODEL_TEMPERATURE=0.0
MAX_ITERATIONS=20
DEFAULT_MAX_RESULTS=10
LOG_LEVEL=INFO
</file>

<file path=".gitignore">
# ===================================================================
# GITIGNORE POUR REGULAI - PROJET LANGGRAPH + MCP
# ===================================================================

# ===================================================================
# PYTHON
# ===================================================================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# ===================================================================
# LANGGRAPH & LANGCHAIN SP√âCIFIQUE
# ===================================================================
# Cl√©s API et configuration sensible
.env.local
.env.production
.env.development
api_keys.txt
secrets.json

# Fichiers de session LangGraph
.langgraph/
langgraph_sessions/
agent_sessions/

# Checkpoints LangGraph
checkpoints/
.checkpoints/

# Cache des mod√®les
model_cache/
.model_cache/

# ===================================================================
# MCP (MODEL CONTEXT PROTOCOL)
# ===================================================================
# Configuration MCP locale
mcp_config.local.json
.mcp/

# Logs MCP
mcp_server.log
mcp_client.log

# ===================================================================
# IDE & √âDITEURS
# ===================================================================
# Visual Studio Code
.vscode/
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json
*.code-workspace

# PyCharm
.idea/
*.iws
*.iml
*.ipr

# Sublime Text
*.sublime-project
*.sublime-workspace

# Vim
*.swp
*.swo
*~

# Emacs
*~
\#*\#
/.emacs.desktop
/.emacs.desktop.lock
*.elc
auto-save-list
tramp
.\#*

# ===================================================================
# SYST√àMES D'EXPLOITATION
# ===================================================================
# Windows
Thumbs.db
Thumbs.db:encryptable
ehthumbs.db
ehthumbs_vista.db
*.stackdump
[Dd]esktop.ini
$RECYCLE.BIN/
*.cab
*.msi
*.msix
*.msm
*.msp
*.lnk

# macOS
.DS_Store
.AppleDouble
.LSOverride
Icon
._*
.DocumentRevisions-V100
.fseventsd
.Spotlight-V100
.TemporaryItems
.Trashes
.VolumeIcon.icns
.com.apple.timemachine.donotpresent
.AppleDB
.AppleDesktop
Network Trash Folder
Temporary Items
.apdisk

# Linux
*~

# ===================================================================
# LOGS & FICHIERS TEMPORAIRES
# ===================================================================
# Logs g√©n√©raux
*.log
logs/
*.log.*

# Fichiers temporaires
tmp/
temp/
*.tmp
*.temp
*.bak
*.backup
*.old

# ===================================================================
# BASE DE DONN√âES & STOCKAGE
# ===================================================================
# SQLite
*.db
*.sqlite
*.sqlite3

# Autres bases de donn√©es
*.mdb
*.accdb

# ===================================================================
# CONFIGURATION SP√âCIFIQUE AU PROJET
# ===================================================================
# Rapports g√©n√©r√©s
tools_validation_report.json
agent_performance_report.json
test_results/

# Fichiers de d√©veloppement temporaire
scratch/
playground/
experiments/

# Configuration locale de d√©veloppement
config.local.py
config.dev.py

# Donn√©es de test sensibles
test_data/private/
sensitive_test_cases/

# ===================================================================
# ARCHIVES & BUILDS
# ===================================================================
*.zip
*.tar.gz
*.tgz
*.rar
*.7z

# ===================================================================
# AUTRES
# ===================================================================
# Fichiers g√©n√©r√©s par certains outils
.DS_Store?
.coverage.*
.nyc_output
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
</file>

<file path=".repomixignore">
repomix.config.json
</file>

<file path="examples/__init__.py">
# Exemples d'utilisation du package RegulAI
</file>

<file path="examples/simple_conversation.py">
#!/usr/bin/env python3
"""
Exemple simple d'utilisation de l'agent RegulAI.

Ce script d√©montre comment utiliser l'agent RegulAI pour une conversation
basique sur le droit fran√ßais.
"""

import os
import sys

# Ajouter le r√©pertoire src au PYTHONPATH pour les imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from regulai.agent import create_agent, run_agent_conversation, stream_agent_conversation
from regulai.config import get_config


def exemple_conversation_simple():
    """Exemple de conversation simple avec l'agent."""
    print("ü§ñ RegulAI - Exemple de Conversation Simple")
    print("=" * 60)
    
    try:
        # V√©rifier la configuration
        config = get_config()
        print(f"üìã Configuration:")
        print(f"   - Mod√®le: {config.model_name}")
        print(f"   - Serveur MCP: {config.mcp_server_url}")
        print(f"   - API Key configur√©e: {'‚úÖ' if config.openai_api_key else '‚ùå'}")
        
        if not config.openai_api_key:
            print("\n‚ùå OPENAI_API_KEY n'est pas configur√©")
            print("Veuillez copier .env.example vers .env et remplir votre cl√© API")
            return
        
        # Cr√©er l'agent
        print(f"\nüèóÔ∏è  Cr√©ation de l'agent...")
        agent = create_agent()
        print("‚úÖ Agent cr√©√© avec succ√®s")
        
        # Messages d'exemple
        messages_test = [
            "Bonjour ! Peux-tu m'expliquer ce qu'est le droit du travail en France ?",
            "Comment fonctionne la recherche dans L√©gifrance ?",
            "Peux-tu rechercher des informations sur les cong√©s pay√©s ?",
        ]
        
        for i, message in enumerate(messages_test, 1):
            print(f"\n" + "‚îÄ" * 60)
            print(f"üí¨ Exemple {i}: {message}")
            print("‚îÄ" * 60)
            
            # Utiliser l'agent pour r√©pondre
            response = run_agent_conversation(
                message, 
                thread_id=f"exemple-{i}",
                agent=agent
            )
            
            print(f"ü§ñ R√©ponse: {response}")
        
        print(f"\n" + "=" * 60)
        print("‚úÖ Tous les exemples ont √©t√© ex√©cut√©s avec succ√®s !")
        
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        print("\nV√©rifiez votre configuration et r√©essayez.")


def exemple_conversation_streaming():
    """Exemple de conversation avec streaming en temps r√©el."""
    print("\nüåä RegulAI - Exemple avec Streaming")
    print("=" * 60)
    
    try:
        # Cr√©er l'agent
        agent = create_agent()
        
        # Message de test pour le streaming
        message = "Peux-tu me donner un aper√ßu complet du Code du travail fran√ßais ?"
        print(f"üí¨ Question: {message}")
        print("ü§ñ R√©ponse en streaming:")
        print("‚îÄ" * 40)
        
        # Utiliser le streaming
        for step in stream_agent_conversation(message, "streaming-example", agent):
            if "messages" in step:
                last_message = step["messages"][-1]
                if hasattr(last_message, 'content') and last_message.content:
                    # Afficher le contenu s'il ne s'agit pas d'appels d'outils
                    if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
                        print(f"üìù {last_message.content}")
                    else:
                        print(f"üîß [Appel d'outils en cours...]")
        
        print("‚îÄ" * 40)
        print("‚úÖ Streaming termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur lors du streaming: {e}")


def main():
    """Fonction principale."""
    print("üöÄ Exemples d'utilisation de RegulAI")
    print("=" * 60)
    
    # V√©rifier les d√©pendances
    try:
        import regulai
        print("‚úÖ Package RegulAI import√© avec succ√®s")
    except ImportError as e:
        print(f"‚ùå Erreur d'import: {e}")
        print("Assurez-vous d'avoir install√© le package avec: pip install -e .")
        return
    
    # Ex√©cuter les exemples
    try:
        exemple_conversation_simple()
        print("\n" + "="*20)
        exemple_conversation_streaming()
        
    except KeyboardInterrupt:
        print("\n\nüëã Arr√™t demand√© par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur g√©n√©rale: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
</file>

<file path="langgraph.json">
{
    "dependencies": ["./src/regulai"],
    "graphs": {
        "regulai_agent": "./src/regulai/agent.py:create_agent"
    },
    "env": ".env"
}
</file>

<file path="PROJET_STRUCTURE.md">
# Structure du Projet RegulAI

## üéØ Objectif

Construire un agent IA avec LangGraph qui interagit avec le serveur MCP L√©gifrance existant. L'architecture suit le pattern client-serveur o√π le serveur MCP s'ex√©cute comme un service ind√©pendant et l'agent LangGraph agit comme un client HTTP.

## üìÅ Structure des Fichiers

```
RegulAI/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ mcp/                    # Serveur MCP existant
‚îÇ   ‚îî‚îÄ‚îÄ agent/                  # üÜï Nouvel agent LangGraph
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py         # Package Python
‚îÇ       ‚îú‚îÄ‚îÄ main.py             # Agent principal (ReAct)
‚îÇ       ‚îú‚îÄ‚îÄ tools.py            # Interface HTTP vers MCP
‚îÇ       ‚îú‚îÄ‚îÄ test_agent.py       # Tests avec outils simul√©s
‚îÇ       ‚îî‚îÄ‚îÄ README.md           # Documentation agent
‚îú‚îÄ‚îÄ pyproject.toml              # ‚úÖ D√©pendances mises √† jour
‚îú‚îÄ‚îÄ README.md                   # Documentation projet
‚îî‚îÄ‚îÄ PROJET_STRUCTURE.md         # Ce fichier
```

## üèóÔ∏è Architecture

L'agent utilise l'**API fonctionnelle de LangGraph** avec les d√©corateurs :
- `@task` : T√¢ches individuelles (call_model, call_tool)
- `@entrypoint` : Workflow principal avec persistance
- **Pattern ReAct** : Reasoning ‚Üí Action ‚Üí Observation

## üì¶ D√©pendances Ajout√©es

```toml
dependencies = [
    "fastmcp @ git+https://github.com/jlowin/fastmcp.git",  # Existant
    "langgraph>=0.2.16",           # üÜï Framework principal
    "langchain-openai>=0.2.0",     # üÜï Provider LLM
    "langchain-core>=0.3.0",       # üÜï Messages et outils
    "httpx>=0.27.0",               # üÜï Client HTTP
    "pydantic>=2.0.0"              # üÜï Validation donn√©es
]
```

## üîß Composants Principaux

### 1. Agent Principal (`main.py`)
- **Architecture ReAct** avec boucle raisonnement/action
- **Persistance** des conversations via MemorySaver
- **Streaming** des r√©ponses en temps r√©el
- **Gestion d'erreurs** robuste

### 2. Interface MCP (`tools.py`)
- **Client HTTP** vers le serveur MCP (localhost:8000)
- **Conversion** outils MCP ‚Üí format LangChain
- **Outils disponibles** :
  - `search_legifrance` : Recherche textes juridiques
  - `get_article` : R√©cup√©ration article par ID
  - `browse_code` : Navigation codes juridiques

### 3. Tests Simul√©s (`test_agent.py`)
- **Tests ind√©pendants** sans serveur MCP
- **Outils mock√©s** avec donn√©es r√©alistes
- **Validation** du pattern ReAct

## üöÄ Utilisation

### Installation
```bash
pip install langgraph langchain-openai langchain-core httpx pydantic
```

### Test rapide
```bash
python src/agent/test_agent.py
```

### Utilisation avec serveur MCP
```python
from src.agent.main import agent
from langchain_core.messages import HumanMessage

# Configuration
config = {"configurable": {"thread_id": "session-123"}}
message = HumanMessage(content="Quelles sont les r√®gles sur les cong√©s pay√©s?")

# Ex√©cution
for step in agent.stream([message], config=config):
    for task_name, result in step.items():
        if task_name != "agent":
            print(f"{task_name}: {result}")
```

## üìã Prochaines √âtapes

### Phase 1 : Tests et Validation ‚úÖ
- [x] Structure du projet cr√©√©e
- [x] Agent LangGraph impl√©ment√©
- [x] Interface HTTP vers MCP
- [x] Tests simul√©s fonctionnels
- [x] Outils avec d√©corateur @tool
- [x] Sch√©mas Pydantic pour validation
- [x] Scripts de validation et exemples

### Phase 2 : Int√©gration MCP
- [ ] D√©marrer le serveur MCP L√©gifrance
- [ ] Tester l'int√©gration compl√®te
- [ ] R√©cup√©ration dynamique des outils MCP
- [ ] Am√©liorer le parsing des r√©ponses

### Phase 3 : Fonctionnalit√©s Avanc√©es
- [ ] Interface web (Streamlit/FastAPI)
- [ ] Cache des requ√™tes fr√©quentes
- [ ] M√©triques et observabilit√©
- [ ] Tests automatis√©s complets

### Phase 4 : Production
- [ ] D√©ploiement containeris√©
- [ ] Configuration via variables d'environnement
- [ ] Monitoring et alertes
- [ ] Documentation utilisateur

## üîç Points d'Attention

1. **Variables d'environnement** : OPENAI_API_KEY requis
2. **Serveur MCP** : Doit √™tre d√©marr√© sur localhost:8000
3. **Format des r√©ponses** : Adapter le parsing selon l'API MCP
4. **Gestion d'erreurs** : Timeout HTTP, erreurs r√©seau
5. **Rate limiting** : Limites API OpenAI et L√©gifrance

## üìö Documentation de R√©f√©rence

- **LangGraph Functional API** : how-tos/react-agent-from-scratch-functional.ipynb
- **Pattern ReAct** : Reasoning and Acting avec outils
- **MCP Protocol** : Model Context Protocol pour l'interop√©rabilit√©
- **L√©gifrance API** : Base de donn√©es juridique fran√ßaise

L'architecture est maintenant pr√™te pour les tests d'int√©gration avec le serveur MCP r√©el ! üéâ
</file>

<file path="pyproject.toml">
[project]
name = "regulai"
version = "0.2.0"
description = "Agent IA ReAct avec int√©gration MCP pour recherche juridique"
authors = [
    {name = "RegulAI Team", email = "team@regulai.com"}
]
license = {text = "MIT"}
readme = "README.md"
requires-python = ">=3.12,<4.0"
dependencies = [
    # "fastmcp @ git+https://github.com/jlowin/fastmcp.git",  # Temporairement comment√©
    "langgraph>=0.2.16",
    "langchain-openai>=0.2.0",
    "langchain-core>=0.3.0",
    "httpx>=0.27.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "streamlit>=1.39.0"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-mock>=3.10.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=1.0.0"
]

[project.scripts]
regulai = "regulai.agent:main"
regulai-validate = "scripts.validate_tools:main"
regulai-web = "streamlit run streamlit_app.py"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"

[tool.black]
line-length = 88
target-version = ['py312']

[tool.isort]
profile = "black"
src_paths = ["src", "scripts", "examples", "tests"]

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
</file>

<file path="scripts/__init__.py">
# Scripts d'administration et de maintenance pour RegulAI
</file>

<file path="scripts/validate_tools.py">
"""
Script de validation des outils LangGraph L√©gifrance.

Ce script v√©rifie que tous les outils sont correctement configur√©s
et peuvent √™tre utilis√©s par l'agent LangGraph.
"""

import json
import os
import sys
from typing import Dict, Any

# Ajouter le r√©pertoire src au PYTHONPATH pour les imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from regulai.tools import get_available_tools, test_mcp_connection, get_mcp_server_info
from regulai.config import get_config


def validate_tool_structure(tool) -> Dict[str, Any]:
    """
    Valide la structure d'un outil LangChain.
    
    Args:
        tool: Outil √† valider
        
    Returns:
        Dictionnaire avec les r√©sultats de validation
    """
    validation = {
        "name": tool.name,
        "has_description": bool(tool.description),
        "has_args_schema": hasattr(tool, 'args_schema') and tool.args_schema is not None,
        "schema_fields": [],
        "errors": []
    }
    
    # V√©rifier le sch√©ma d'arguments
    if validation["has_args_schema"]:
        try:
            schema = tool.args_schema.schema()
            validation["schema_fields"] = list(schema.get("properties", {}).keys())
            validation["schema_valid"] = True
        except Exception as e:
            validation["errors"].append(f"Erreur sch√©ma: {e}")
            validation["schema_valid"] = False
    else:
        validation["errors"].append("Pas de sch√©ma d'arguments d√©fini")
        validation["schema_valid"] = False
    
    # V√©rifier que l'outil est invocable
    try:
        # Test avec des arguments vides pour v√©rifier la structure
        tool.get_input_schema()
        validation["invocable"] = True
    except Exception as e:
        validation["errors"].append(f"Outil non invocable: {e}")
        validation["invocable"] = False
    
    return validation


def test_tool_invocation(tool, test_args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Teste l'invocation d'un outil avec des arguments de test.
    
    Args:
        tool: Outil √† tester
        test_args: Arguments de test
        
    Returns:
        R√©sultats du test
    """
    test_result = {
        "tool_name": tool.name,
        "test_args": test_args,
        "success": False,
        "response": None,
        "error": None,
        "response_length": 0
    }
    
    try:
        response = tool.invoke(test_args)
        test_result["success"] = True
        test_result["response"] = response
        test_result["response_length"] = len(str(response))
        
        # V√©rifier que la r√©ponse n'est pas une erreur
        response_str = str(response).lower()
        if any(error_keyword in response_str for error_keyword in ["erreur", "error", "connexion", "timeout"]):
            test_result["warning"] = "La r√©ponse semble contenir une erreur"
            
    except Exception as e:
        test_result["error"] = str(e)
    
    return test_result


def validate_all_tools():
    """Valide tous les outils disponibles."""
    print("üîç Validation des outils LangGraph L√©gifrance")
    print("=" * 60)
    
    # 1. Tester la connexion MCP
    print("\nüì° Test de connexion MCP")
    print("-" * 30)
    
    mcp_available = test_mcp_connection()
    if mcp_available:
        print("‚úÖ Serveur MCP accessible")
        server_info = get_mcp_server_info()
        if server_info:
            print(f"üìä Info serveur: {json.dumps(server_info, indent=2)}")
    else:
        print("‚ö†Ô∏è  Serveur MCP non accessible - les tests d'invocation pourraient √©chouer")
    
    # 2. R√©cup√©rer et valider les outils
    print("\nüîß Validation de la structure des outils")
    print("-" * 40)
    
    tools = get_available_tools()
    print(f"Nombre d'outils trouv√©s: {len(tools)}")
    
    validation_results = []
    for tool in tools:
        validation = validate_tool_structure(tool)
        validation_results.append(validation)
        
        # Afficher les r√©sultats
        print(f"\nüìù Outil: {validation['name']}")
        print(f"   Description: {'‚úÖ' if validation['has_description'] else '‚ùå'}")
        print(f"   Sch√©ma args: {'‚úÖ' if validation['has_args_schema'] else '‚ùå'}")
        print(f"   Invocable: {'‚úÖ' if validation['invocable'] else '‚ùå'}")
        print(f"   Champs: {validation['schema_fields']}")
        
        if validation['errors']:
            print(f"   ‚ö†Ô∏è  Erreurs: {validation['errors']}")
    
    # 3. Tests d'invocation
    print("\nüöÄ Tests d'invocation des outils")
    print("-" * 35)
    
    # Arguments de test pour chaque outil
    test_cases = {
        "search_legifrance": {"query": "test validation", "max_results": 1},
        "get_article": {"article_id": "L3141-1"},
        "browse_code": {"code_name": "Code du travail", "section": "L3141"}
    }
    
    invocation_results = []
    for tool in tools:
        if tool.name in test_cases:
            test_args = test_cases[tool.name]
            result = test_tool_invocation(tool, test_args)
            invocation_results.append(result)
            
            # Afficher les r√©sultats
            print(f"\nüß™ Test {tool.name}")
            print(f"   Arguments: {test_args}")
            print(f"   Succ√®s: {'‚úÖ' if result['success'] else '‚ùå'}")
            print(f"   Longueur r√©ponse: {result['response_length']} chars")
            
            if result['error']:
                print(f"   ‚ùå Erreur: {result['error']}")
            elif result.get('warning'):
                print(f"   ‚ö†Ô∏è  {result['warning']}")
            elif result['success']:
                # Afficher un aper√ßu de la r√©ponse
                response_preview = str(result['response'])[:100]
                print(f"   üìÑ Aper√ßu: {response_preview}...")
    
    # 4. R√©sum√© des r√©sultats
    print("\nüìä R√©sum√© de la validation")
    print("=" * 30)
    
    total_tools = len(tools)
    valid_structure = sum(1 for v in validation_results if v['invocable'] and v['has_args_schema'])
    successful_invocations = sum(1 for r in invocation_results if r['success'])
    
    print(f"Outils totaux: {total_tools}")
    print(f"Structure valide: {valid_structure}/{total_tools}")
    print(f"Invocations r√©ussies: {successful_invocations}/{len(test_cases)}")
    
    # Statut global
    if valid_structure == total_tools and (not mcp_available or successful_invocations > 0):
        print("\n‚úÖ Validation globale: SUCC√àS")
        return True
    else:
        print("\n‚ùå Validation globale: √âCHEC")
        if valid_structure != total_tools:
            print("   - Probl√®mes de structure des outils")
        if mcp_available and successful_invocations == 0:
            print("   - Aucune invocation r√©ussie malgr√© serveur MCP disponible")
        return False


def generate_tools_report():
    """G√©n√®re un rapport d√©taill√© des outils."""
    print("\nüìã G√©n√©ration du rapport des outils")
    print("-" * 40)
    
    tools = get_available_tools()
    
    report = {
        "timestamp": str(type(None).__module__ == "builtins" and True),  # Simple timestamp
        "mcp_server_available": test_mcp_connection(),
        "tools_count": len(tools),
        "tools": []
    }
    
    for tool in tools:
        tool_info = {
            "name": tool.name,
            "description": tool.description,
            "schema": None
        }
        
        # Extraire le sch√©ma
        try:
            if hasattr(tool, 'args_schema') and tool.args_schema:
                schema = tool.args_schema.schema()
                tool_info["schema"] = {
                    "properties": schema.get("properties", {}),
                    "required": schema.get("required", [])
                }
        except Exception as e:
            tool_info["schema_error"] = str(e)
        
        report["tools"].append(tool_info)
    
    # Sauvegarder le rapport
    try:
        with open("tools_validation_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print("‚úÖ Rapport sauvegard√©: tools_validation_report.json")
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde rapport: {e}")
    
    return report


def main():
    """Fonction principale de validation."""
    try:
        # Validation compl√®te
        success = validate_all_tools()
        
        # G√©n√©ration du rapport
        generate_tools_report()
        
        # Statut final
        if success:
            print("\nüéâ Validation termin√©e avec succ√®s!")
            print("Les outils sont pr√™ts pour l'utilisation avec l'agent LangGraph.")
        else:
            print("\n‚ö†Ô∏è  Validation termin√©e avec des probl√®mes.")
            print("V√©rifiez les erreurs ci-dessus avant d'utiliser l'agent.")
        
        return success
        
    except Exception as e:
        print(f"\n‚ùå Erreur durant la validation: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    main()
</file>

<file path="services/__init__.py">
# Services externes autonomes
</file>

<file path="services/mcp/__init__.py">
# Service MCP autonome
</file>

<file path="services/mcp/main.py">
# src/main.py
import os
import signal
import sys
from dotenv import load_dotenv
from utils import create_mcp_from_openapi

# Charger les variables d'environnement depuis un fichier .env
load_dotenv()

print("Cr√©ation du serveur MCP √† partir de la sp√©cification OpenAPI...")

# Cr√©er l'instance du serveur en utilisant notre fonction utilitaire
mcp = create_mcp_from_openapi()

print(f"Serveur MCP '{mcp.name}' cr√©√© avec succ√®s. D√©marrage...")

def signal_handler(signum, frame):
    """
    Gestionnaire de signal pour un arr√™t propre du serveur
    """
    print("\nüõë Arr√™t du serveur demand√©...")
    print("üëã Serveur arr√™t√© proprement!")
    sys.exit(0)

# Le bloc d'ex√©cution principal
if __name__ == "__main__":
    # Configurer le gestionnaire de signal pour Ctrl+C (SIGINT)
    signal.signal(signal.SIGINT, signal_handler)
    
    # R√©cup√©rer l'h√¥te et le port depuis les variables d'environnement, avec des valeurs par d√©faut
    host = os.getenv("HOST", "127.0.0.1")
    port = int(os.getenv("PORT", "8000"))

    try:
        print(f"üöÄ Serveur d√©marr√© sur http://{host}:{port}")
        print("üí° Appuyez sur Ctrl+C pour arr√™ter le serveur")
        
        # Lancer le serveur avec le transport HTTP recommand√© pour ce cas d'usage
        # Voir : https://gofastmcp.com/deployment/running-server#streamable-http
        mcp.run(transport="streamable-http", host=host, port=port)
    except KeyboardInterrupt:
        # Au cas o√π le signal handler ne capture pas l'interruption
        print("\nüõë Arr√™t du serveur demand√©...")
        print("üëã Serveur arr√™t√© proprement!")
        sys.exit(0)
    except Exception as e:
        print(f"‚ùå Erreur lors du d√©marrage du serveur : {e}")
        sys.exit(1)
</file>

<file path="services/mcp/utils.py">
# src/utils.py
import os
import json
import httpx
from pathlib import Path
from fastmcp import FastMCP

def get_access_token() -> str:
    """
    R√©cup√®re un jeton d'acc√®s (access token) aupr√®s du serveur OAuth de PISTE
    en utilisant les identifiants client (Client Credentials).
    """
    client_id = os.getenv("OAUTH_CLIENT_ID")
    client_secret = os.getenv("OAUTH_CLIENT_SECRET")
    token_url = os.getenv("OAUTH_TOKEN_URL")

    if not all([client_id, client_secret, token_url]):
        raise ValueError("Les variables d'environnement OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, et OAUTH_TOKEN_URL sont requises.")

    # Assurance pour le type checker que token_url n'est pas None apr√®s la v√©rification
    assert token_url is not None

    print("Demande d'un jeton d'acc√®s (access token) √† PISTE...")
    
    # Le corps de la requ√™te pour obtenir le token
    data = {
        "grant_type": "client_credentials",
        "client_id": client_id,
        "client_secret": client_secret,
    }

    try:
        # On utilise un client synchrone ici car c'est une action unique au d√©marrage
        response = httpx.post(token_url, data=data)
        response.raise_for_status()  # L√®ve une exception si la requ√™te √©choue (ex: 401, 500)
        
        token_data = response.json()
        access_token = token_data.get("access_token")

        if not access_token:
            raise ValueError("La r√©ponse du serveur OAuth ne contient pas d'access_token.")
            
        print("Jeton d'acc√®s obtenu avec succ√®s !")
        return access_token

    except httpx.HTTPStatusError as e:
        print(f"Erreur HTTP lors de l'obtention du token : {e.response.status_code}")
        print(f"R√©ponse du serveur : {e.response.text}")
        raise
    except Exception as e:
        print(f"Une erreur inattendue est survenue lors de l'obtention du token : {e}")
        raise


def create_mcp_from_openapi() -> FastMCP:
    """
    Cr√©e une instance de serveur FastMCP √† partir de la sp√©cification OpenAPI de L√©gifrance.
    """
    # 1. R√©cup√©rer l'URL de base de l'API
    api_base_url = os.getenv("API_BASE_URL")
    if not api_base_url:
        raise ValueError("La variable d'environnement API_BASE_URL n'est pas d√©finie.")

    # 2. Obtenir le jeton d'acc√®s via OAuth 2.0
    access_token = get_access_token()

    # 3. Charger le fichier de sp√©cification OpenAPI
    project_root = Path(__file__).parent.parent.parent
    spec_path = project_root / "openapi.json"
    if not spec_path.exists():
        raise FileNotFoundError(f"Fichier openapi.json non trouv√© √† l'emplacement : {spec_path}")

    with open(spec_path, "r", encoding="utf-8") as f:
        openapi_spec = json.load(f)

    # 4. Configurer le client HTTP avec le jeton d'acc√®s obtenu
    headers = {
        "Authorization": f"Bearer {access_token}",
        # L'API PISTE requiert aussi ce header pour les appels
        "accept": "application/json",
    }
    
    http_client = httpx.AsyncClient(base_url=api_base_url, headers=headers)

    # 5. G√©n√©rer le serveur MCP
    server_name = openapi_spec.get("info", {}).get("title", "L√©gifrance MCP Server")
    
    mcp_server = FastMCP.from_openapi(
        openapi_spec=openapi_spec,
        client=http_client,
        name=server_name,
        timeout=30.0
    )

    return mcp_server
</file>

<file path="src/regulai/__init__.py">
"""
RegulAI - Agent IA ReAct avec int√©gration MCP pour recherche juridique.

Ce package contient l'agent LangGraph principal et ses composants.
"""

__version__ = "0.2.0"
__author__ = "RegulAI Team"

from .agent import create_agent
from .config import RegulAIConfig

__all__ = ["create_agent", "RegulAIConfig"]
</file>

<file path="src/regulai/agent.py">
"""
Agent RegulAI principal avec architecture StateGraph.

Ce module assemble les n≈ìuds d√©finis dans graph.py pour cr√©er l'agent LangGraph
complet avec persistance et streaming.
"""

from typing import Optional, Any, Dict
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.graph import CompiledGraph

from .config import get_config
from .graph import AgentState, call_model, call_tools, should_continue


def create_agent(checkpointer: Optional[MemorySaver] = None) -> CompiledGraph:
    """
    Cr√©e et compile l'agent RegulAI avec architecture StateGraph.
    
    Args:
        checkpointer: Checkpointer pour la persistance (utilise MemorySaver par d√©faut)
        
    Returns:
        Agent LangGraph compil√© et pr√™t √† utiliser
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    # Valider la configuration
    config = get_config()
    
    # Cr√©er le StateGraph avec l'√©tat AgentState
    workflow = StateGraph(AgentState)
    
    # Ajouter les n≈ìuds
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", call_tools)
    
    # D√©finir le point d'entr√©e
    workflow.set_entry_point("agent")
    
    # Ajouter les ar√™tes conditionnelles
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "__end__": END,
        },
    )
    
    # Apr√®s les outils, retourner √† l'agent
    workflow.add_edge("tools", "agent")
    
    # Compiler le graphe avec un checkpointer
    if checkpointer is None:
        checkpointer = MemorySaver()
    
    compiled_graph = workflow.compile(checkpointer=checkpointer)
    
    return compiled_graph


def run_agent_conversation(
    message: str, 
    thread_id: str = "default-session",
    agent: Optional[CompiledGraph] = None
) -> str:
    """
    Lance une conversation avec l'agent RegulAI.
    
    Args:
        message: Message de l'utilisateur
        thread_id: ID de session pour la persistance
        agent: Instance de l'agent (cr√©√©e automatiquement si None)
        
    Returns:
        R√©ponse finale de l'agent
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    if agent is None:
        agent = create_agent()
    
    # Cr√©er le message utilisateur
    user_message = HumanMessage(content=message)
    
    # Configuration pour la persistance
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
    
    # Invoquer l'agent
    result = agent.invoke(
        {"messages": [user_message]},
        config=config
    )
    
    # R√©cup√©rer la r√©ponse finale
    if result and "messages" in result:
        last_message = result["messages"][-1]
        if hasattr(last_message, 'content'):
            return last_message.content
    
    return "Aucune r√©ponse de l'agent"


def stream_agent_conversation(
    message: str,
    thread_id: str = "default-session", 
    agent: Optional[CompiledGraph] = None
):
    """
    Lance une conversation avec streaming avec l'agent RegulAI.
    
    Args:
        message: Message de l'utilisateur
        thread_id: ID de session pour la persistance
        agent: Instance de l'agent (cr√©√©e automatiquement si None)
        
    Yields:
        √âtapes interm√©diaires et r√©ponse finale de l'agent
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    if agent is None:
        agent = create_agent()
    
    # Cr√©er le message utilisateur
    user_message = HumanMessage(content=message)
    
    # Configuration pour la persistance
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
    
    # Streamer les √©tapes avec mode "updates" pour capturer les appels d'outils
    # et les transitions entre n≈ìuds
    for step in agent.stream(
        {"messages": [user_message]},
        config=config,
        stream_mode="updates"
    ):
        yield step


def stream_agent_conversation_with_tokens(
    message: str,
    thread_id: str = "default-session", 
    agent: Optional[CompiledGraph] = None
):
    """
    Lance une conversation avec streaming token par token avec l'agent RegulAI.
    
    Cette fonction utilise le mode "messages" pour obtenir les tokens individuels
    du mod√®le de langage, permettant un affichage progressif plus granulaire.
    
    Args:
        message: Message de l'utilisateur
        thread_id: ID de session pour la persistance
        agent: Instance de l'agent (cr√©√©e automatiquement si None)
        
    Yields:
        Tokens et m√©tadonn√©es de streaming
        
    Raises:
        ValueError: Si la configuration est invalide
    """
    if agent is None:
        agent = create_agent()
    
    # Cr√©er le message utilisateur
    user_message = HumanMessage(content=message)
    
    # Configuration pour la persistance
    config: RunnableConfig = {"configurable": {"thread_id": thread_id}}
    
    # Streamer avec mode "messages" pour obtenir les tokens individuels
    for token, metadata in agent.stream(
        {"messages": [user_message]},
        config=config,
        stream_mode="messages"
    ):
        yield token, metadata


def main():
    """
    Fonction principale pour tester l'agent RegulAI.
    
    Lance une conversation de test avec l'agent et affiche les r√©sultats.
    """
    try:
        print("ü§ñ Agent RegulAI - Test de d√©marrage")
        print("=" * 50)
        
        # V√©rifier la configuration
        config = get_config()
        if not config.openai_api_key:
            print("‚ùå Erreur: OPENAI_API_KEY non configur√©")
            print("Copiez .env.example vers .env et remplissez votre cl√© API OpenAI")
            return
        
        print(f"‚úÖ Configuration valid√©e")
        print(f"   - Mod√®le: {config.model_name}")
        print(f"   - Serveur MCP: {config.mcp_server_url}")
        print(f"   - Temperature: {config.model_temperature}")
        
        # Cr√©er l'agent
        print("\nüìù Cr√©ation de l'agent...")
        agent = create_agent()
        print("‚úÖ Agent cr√©√© avec succ√®s")
        
        # Message de test
        test_message = "Bonjour ! Peux-tu m'aider √† rechercher des informations sur les cong√©s pay√©s en France ?"
        print(f"\nüë§ Utilisateur: {test_message}")
        print("\nü§ñ Agent RegulAI:")
        print("-" * 30)
        
        # Lancer la conversation avec streaming
        for step in stream_agent_conversation(test_message, "test-session", agent):
            if "messages" in step:
                last_message = step["messages"][-1]
                if hasattr(last_message, 'content') and last_message.content:
                    # Afficher seulement les nouveaux contenus (pas les appels d'outils)
                    if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
                        print(f"üí¨ {last_message.content}")
        
        print("\n" + "=" * 50)
        print("‚úÖ Test termin√© avec succ√®s !")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors du test: {e}")
        print("\nV√©rifiez votre configuration :")
        print("1. OPENAI_API_KEY est d√©fini dans .env")
        print("2. Le serveur MCP est d√©marr√© (optionnel pour ce test)")


if __name__ == "__main__":
    main()
</file>

<file path="src/regulai/config.py">
"""
Configuration centralis√©e pour RegulAI avec validation Pydantic.

Ce module g√®re toutes les variables d'environnement et param√®tres de configuration
du projet de mani√®re type-safe avec validation automatique.
"""

from typing import Optional
from pydantic import Field
from pydantic_settings import BaseSettings


class RegulAIConfig(BaseSettings):
    """Configuration centralis√©e avec validation Pydantic."""
    
    # ==========================================
    # Configuration Serveur MCP  
    # ==========================================
    mcp_server_url: str = Field(
        default="http://localhost:8000",
        description="URL du serveur MCP L√©gifrance"
    )
    mcp_timeout: int = Field(
        default=30,
        description="Timeout en secondes pour les requ√™tes MCP"
    )
    
    # ==========================================
    # Configuration LLM
    # ==========================================
    openai_api_key: Optional[str] = Field(
        default=None,
        description="Cl√© API OpenAI pour le mod√®le de langage"
    )
    model_name: str = Field(
        default="gpt-4o-mini",
        description="Nom du mod√®le OpenAI √† utiliser"
    )
    model_temperature: float = Field(
        default=0.0,
        ge=0.0,
        le=2.0,
        description="Temp√©rature du mod√®le (0.0 = d√©terministe, 2.0 = tr√®s cr√©atif)"
    )
    
    # ==========================================
    # Configuration Agent
    # ==========================================
    max_iterations: int = Field(
        default=20,
        ge=1,
        le=100,
        description="Nombre maximum d'it√©rations pour l'agent ReAct"
    )
    default_max_results: int = Field(
        default=10,
        ge=1,
        le=50,
        description="Nombre par d√©faut de r√©sultats pour les recherches"
    )
    
    # ==========================================
    # Configuration Logging
    # ==========================================
    log_level: str = Field(
        default="INFO",
        description="Niveau de logging (DEBUG, INFO, WARNING, ERROR, CRITICAL)"
    )
    log_format: str = Field(
        default="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        description="Format des logs"
    )
    
    # ==========================================
    # Configuration Persistance
    # ==========================================
    thread_id: Optional[str] = Field(
        default=None,
        description="ID de thread par d√©faut pour la persistance des conversations"
    )
    
    model_config = {
        "env_file": ".env",
        "env_file_encoding": "utf-8",
        "case_sensitive": False,
        "validate_assignment": True,
        "env_prefix": "",
    }
    
    def validate_openai_key(self) -> str:
        """Valide que la cl√© OpenAI est pr√©sente."""
        if not self.openai_api_key:
            raise ValueError(
                "OPENAI_API_KEY est requis. "
                "D√©finissez la variable d'environnement ou copiez .env.example vers .env"
            )
        return self.openai_api_key


# Fonction pour cr√©er une instance de configuration avec validation
def create_config() -> RegulAIConfig:
    """
    Cr√©e et valide une instance de configuration.
    
    Returns:
        Instance configur√©e et valid√©e de RegulAIConfig
        
    Raises:
        ValueError: Si une configuration requise est manquante
    """
    config = RegulAIConfig()
    # La validation de la cl√© OpenAI sera faite lors de la cr√©ation de l'agent
    return config


# Instance globale de configuration (lazy loading)
_config: Optional[RegulAIConfig] = None


def get_config() -> RegulAIConfig:
    """
    R√©cup√®re l'instance de configuration globale.
    
    Returns:
        Instance configur√©e de RegulAIConfig
    """
    global _config
    if _config is None:
        _config = create_config()
    return _config


def reload_config() -> RegulAIConfig:
    """
    Recharge la configuration depuis les variables d'environnement.
    
    Utile pour les tests ou les changements de configuration √† l'ex√©cution.
    
    Returns:
        Nouvelle instance de RegulAIConfig
    """
    global _config
    _config = create_config()
    return _config
</file>

<file path="src/regulai/graph.py">
"""
D√©finition de l'√©tat et des n≈ìuds du graphe LangGraph pour l'agent RegulAI.

Ce module contient la structure de donn√©es de l'√©tat de l'agent et les fonctions
des n≈ìuds qui composent le workflow ReAct.
"""

from typing import TypedDict, Literal, List, Dict, Any
from langchain_core.messages import BaseMessage, ToolMessage, AIMessage
from langchain_openai import ChatOpenAI

from .config import get_config
from .tools import get_available_tools


# =============================================================================
# √âTAT DU GRAPHE (StateGraph)
# =============================================================================

class AgentState(TypedDict):
    """
    √âtat de l'agent RegulAI pour le StateGraph.
    
    Attributes:
        messages: Liste des messages de la conversation
    """
    messages: List[BaseMessage]


# =============================================================================
# CONFIGURATION ET OUTILS
# =============================================================================

def get_configured_model() -> ChatOpenAI:
    """
    Cr√©e une instance configur√©e du mod√®le OpenAI.
    
    Returns:
        Instance configur√©e de ChatOpenAI
        
    Raises:
        ValueError: Si la cl√© API OpenAI n'est pas configur√©e
    """
    config = get_config()
    
    # Valider la cl√© API
    if not config.openai_api_key:
        raise ValueError(
            "OPENAI_API_KEY est requis. "
            "D√©finissez la variable d'environnement ou copiez .env.example vers .env"
        )
    
    # D√©finir la variable d'environnement si elle n'est pas d√©j√† d√©finie
    import os
    if not os.environ.get("OPENAI_API_KEY"):
        os.environ["OPENAI_API_KEY"] = config.openai_api_key
    
    return ChatOpenAI(
        model=config.model_name,
        temperature=config.model_temperature
    )


def get_tools_dict() -> Dict[str, Any]:
    """
    R√©cup√®re les outils disponibles sous forme de dictionnaire.
    
    Returns:
        Dictionnaire des outils index√©s par nom
    """
    tools = get_available_tools()
    return {tool.name: tool for tool in tools}


# =============================================================================
# N≈íUDS DU GRAPHE
# =============================================================================

def call_model(state: AgentState) -> Dict[str, List[BaseMessage]]:
    """
    N≈ìud qui appelle le mod√®le de langage avec les messages actuels.
    
    Args:
        state: √âtat actuel de l'agent
        
    Returns:
        Dictionnaire avec la nouvelle liste de messages incluant la r√©ponse du mod√®le
    """
    model = get_configured_model()
    tools = get_available_tools()
    
    # Lier les outils au mod√®le et invoquer
    response = model.bind_tools(tools).invoke(state["messages"])
    
    # Retourner l'√©tat mis √† jour avec la r√©ponse du mod√®le
    return {"messages": [response]}


def call_tools(state: AgentState) -> Dict[str, List[BaseMessage]]:
    """
    N≈ìud qui ex√©cute les outils appel√©s par le mod√®le.
    
    Args:
        state: √âtat actuel de l'agent
        
    Returns:
        Dictionnaire avec les messages des r√©sultats des outils
    """
    # R√©cup√©rer le dernier message (doit √™tre un AIMessage avec tool_calls)
    last_message = state["messages"][-1]
    
    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:
        return {"messages": []}
    
    # R√©cup√©rer les outils disponibles
    tools_dict = get_tools_dict()
    tool_messages = []
    
    # Ex√©cuter chaque outil appel√©
    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        
        if tool_name in tools_dict:
            tool = tools_dict[tool_name]
            try:
                # Invoquer l'outil avec ses arguments
                result = tool.invoke(tool_call["args"])
                
                # Cr√©er un ToolMessage avec le r√©sultat
                tool_message = ToolMessage(
                    content=str(result),
                    tool_call_id=tool_call["id"]
                )
                tool_messages.append(tool_message)
                
            except Exception as e:
                # En cas d'erreur, cr√©er un message d'erreur
                error_message = ToolMessage(
                    content=f"Erreur lors de l'ex√©cution de {tool_name}: {e}",
                    tool_call_id=tool_call["id"]
                )
                tool_messages.append(error_message)
        else:
            # Outil non trouv√©
            error_message = ToolMessage(
                content=f"Outil '{tool_name}' non disponible",
                tool_call_id=tool_call["id"]
            )
            tool_messages.append(error_message)
    
    return {"messages": tool_messages}


# =============================================================================
# FONCTIONS CONDITIONNELLES
# =============================================================================

def should_continue(state: AgentState) -> Literal["tools", "__end__"]:
    """
    D√©termine si l'agent doit continuer avec les outils ou terminer.
    
    Args:
        state: √âtat actuel de l'agent
        
    Returns:
        "tools" si des outils doivent √™tre ex√©cut√©s, "__end__" pour terminer
    """
    # R√©cup√©rer le dernier message
    last_message = state["messages"][-1]
    
    # Si c'est un AIMessage avec des appels d'outils, continuer
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "tools"
    
    # Sinon, terminer
    return "__end__"


# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

def validate_state(state: AgentState) -> bool:
    """
    Valide que l'√©tat de l'agent est correct.
    
    Args:
        state: √âtat √† valider
        
    Returns:
        True si l'√©tat est valide, False sinon
    """
    if not isinstance(state, dict):
        return False
    
    if "messages" not in state:
        return False
    
    if not isinstance(state["messages"], list):
        return False
    
    # V√©rifier que tous les √©l√©ments sont des BaseMessage
    for msg in state["messages"]:
        if not isinstance(msg, BaseMessage):
            return False
    
    return True


def get_last_ai_message(state: AgentState) -> AIMessage | None:
    """
    R√©cup√®re le dernier message AI de l'√©tat.
    
    Args:
        state: √âtat de l'agent
        
    Returns:
        Le dernier AIMessage ou None si aucun trouv√©
    """
    for message in reversed(state["messages"]):
        if isinstance(message, AIMessage):
            return message
    return None
</file>

<file path="src/regulai/tools.py">
"""
Outils pour interagir avec le serveur MCP L√©gifrance via HTTP.

Ce module contient la classe MCPClient pour la communication HTTP et les outils 
LangChain d√©cor√©s avec @tool pour l'int√©gration avec l'agent LangGraph.
"""

import httpx
from typing import List, Dict, Any, Optional
from langchain_core.tools import tool
from pydantic import BaseModel, Field

from .config import get_config


# =============================================================================
# SCH√âMAS PYDANTIC POUR VALIDATION DES ARGUMENTS
# =============================================================================

class SearchParams(BaseModel):
    """Recherche des textes juridiques dans la base de donn√©es L√©gifrance."""
    query: str = Field(
        description="Requ√™te de recherche en fran√ßais (ex: 'cong√©s pay√©s', 'droit du travail')"
    )
    max_results: int = Field(
        default=10,
        description="Nombre maximum de r√©sultats √† retourner"
    )


class ArticleParams(BaseModel):
    """Param√®tres pour r√©cup√©rer un article sp√©cifique."""
    article_id: str = Field(
        description="Identifiant de l'article (ex: 'LEGIARTI000006900846' ou 'L3141-1')"
    )


class BrowseCodeParams(BaseModel):
    """Param√®tres pour naviguer dans un code juridique."""
    code_name: str = Field(
        description="Nom du code juridique (ex: 'Code du travail', 'Code civil')"
    )
    section: Optional[str] = Field(
        default=None,
        description="Section sp√©cifique √† explorer (optionnel)"
    )


# =============================================================================
# CLIENT MCP POUR COMMUNICATION HTTP
# =============================================================================

class MCPClient:
    """
    Client HTTP pour communiquer avec le serveur MCP L√©gifrance.
    
    Cette classe encapsule toute la logique de communication HTTP avec le serveur MCP,
    permettant d'abstraire les d√©tails du protocole MCP pour les outils.
    """
    
    def __init__(self, server_url: Optional[str] = None, timeout: Optional[int] = None):
        """
        Initialise le client MCP.
        
        Args:
            server_url: URL du serveur MCP (utilise la config par d√©faut si None)
            timeout: Timeout en secondes (utilise la config par d√©faut si None)
        """
        config = get_config()
        self.server_url = server_url or config.mcp_server_url
        self.timeout = timeout or config.mcp_timeout
    
    def call_tool(self, tool_name: str, tool_args: Dict[str, Any]) -> str:
        """
        Appelle un outil sur le serveur MCP via HTTP.
        
        Args:
            tool_name: Nom de l'outil MCP √† appeler
            tool_args: Arguments √† passer √† l'outil
        
        Returns:
            R√©ponse de l'outil sous forme de cha√Æne format√©e
            
        Raises:
            Exception: En cas d'erreur de communication ou de serveur
        """
        try:
            # Pr√©parer la requ√™te selon le format attendu par le serveur MCP
            payload = {
                "method": "tools/call",
                "params": {
                    "name": tool_name,
                    "arguments": tool_args
                }
            }
            
            # Faire la requ√™te HTTP au serveur MCP
            with httpx.Client(timeout=self.timeout) as client:
                response = client.post(
                    f"{self.server_url}/invoke",  # Point d'entr√©e principal du serveur
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return self._parse_mcp_response(result)
                else:
                    return self._handle_error_response(response)
                    
        except httpx.RequestError as e:
            return f"Erreur de connexion au serveur MCP ({self.server_url}): {e}"
        except Exception as e:
            return f"Erreur lors de l'appel de l'outil {tool_name}: {e}"
    
    def _parse_mcp_response(self, result: Dict[str, Any]) -> str:
        """
        Parse la r√©ponse du serveur MCP selon son format.
        
        Args:
            result: R√©ponse JSON du serveur MCP
            
        Returns:
            Contenu format√© de la r√©ponse
        """
        # Parser la r√©ponse MCP selon son format
        if "result" in result:
            content = result["result"]
            
            # Si la r√©ponse contient du contenu structur√©
            if isinstance(content, dict) and "content" in content:
                content_data = content["content"]
                
                # G√©rer les r√©ponses avec liste de contenus
                if isinstance(content_data, list) and len(content_data) > 0:
                    first_item = content_data[0]
                    if isinstance(first_item, dict) and "text" in first_item:
                        return first_item["text"]
                    else:
                        return str(first_item)
                else:
                    return str(content_data)
            else:
                return str(content)
        else:
            return f"R√©ponse inattendue du serveur MCP: {result}"
    
    def _handle_error_response(self, response: httpx.Response) -> str:
        """
        G√®re les r√©ponses d'erreur HTTP.
        
        Args:
            response: R√©ponse HTTP d'erreur
            
        Returns:
            Message d'erreur format√©
        """
        error_msg = f"Erreur HTTP {response.status_code}"
        try:
            error_detail = response.json()
            error_msg += f": {error_detail}"
        except:
            error_msg += f": {response.text}"
        return error_msg
    
    def test_connection(self) -> bool:
        """
        Teste la connexion au serveur MCP.
        
        Returns:
            True si la connexion fonctionne, False sinon
        """
        try:
            with httpx.Client(timeout=self.timeout) as client:
                response = client.get(f"{self.server_url}/health")
                return response.status_code == 200
        except:
            return False
    
    def get_server_info(self) -> Optional[Dict[str, Any]]:
        """
        R√©cup√®re les informations du serveur MCP.
        
        Returns:
            Informations du serveur ou None en cas d'erreur
        """
        try:
            with httpx.Client(timeout=self.timeout) as client:
                response = client.get(f"{self.server_url}/info")
                if response.status_code == 200:
                    return response.json()
        except:
            pass
        return None


# Instance globale du client MCP
_mcp_client: Optional[MCPClient] = None


def get_mcp_client() -> MCPClient:
    """
    R√©cup√®re l'instance globale du client MCP.
    
    Returns:
        Instance du client MCP
    """
    global _mcp_client
    if _mcp_client is None:
        _mcp_client = MCPClient()
    return _mcp_client


# =============================================================================
# OUTILS L√âGIFRANCE AVEC D√âCORATEUR @tool
# =============================================================================

@tool("search_legifrance", args_schema=SearchParams, parse_docstring=True)
def search_legifrance(query: str, max_results: int = 10) -> str:
    """
    Recherche des textes juridiques dans la base de donn√©es L√©gifrance.
    
    Cet outil permet de chercher des lois, d√©crets, codes, jurisprudence et autres 
    textes juridiques fran√ßais. Utilisez des termes de recherche en fran√ßais.
    
    Args:
        query: Requ√™te de recherche en fran√ßais (ex: "cong√©s pay√©s", "licenciement √©conomique")
        max_results: Nombre maximum de r√©sultats √† retourner (d√©faut: 10)
        
    Returns:
        Liste format√©e des textes juridiques trouv√©s avec leurs r√©f√©rences
    """
    client = get_mcp_client()
    return client.call_tool("search_legifrance", {
        "query": query,
        "max_results": max_results
    })


@tool("get_article", args_schema=ArticleParams, parse_docstring=True)  
def get_article(article_id: str) -> str:
    """
    R√©cup√®re le contenu complet d'un article juridique sp√©cifique.
    
    Permet d'obtenir le texte int√©gral d'un article de loi, de code ou de d√©cret
    √† partir de son identifiant L√©gifrance ou de sa r√©f√©rence standard.
    
    Args:
        article_id: Identifiant de l'article (ex: "LEGIARTI000006900846", "L3141-1", "R1234-5")
        
    Returns:
        Texte complet de l'article avec ses m√©tadonn√©es (version, dates, etc.)
    """
    client = get_mcp_client()
    return client.call_tool("get_article", {
        "article_id": article_id
    })


@tool("browse_code", args_schema=BrowseCodeParams, parse_docstring=True)
def browse_code(code_name: str, section: Optional[str] = None) -> str:
    """
    Navigate dans la structure hi√©rarchique d'un code juridique fran√ßais.
    
    Permet d'explorer l'organisation d'un code (livres, titres, chapitres, sections)
    et d'obtenir la liste des articles dans une section donn√©e.
    
    Args:
        code_name: Nom du code juridique (ex: "Code du travail", "Code civil", "Code p√©nal")
        section: Section sp√©cifique √† explorer (ex: "L3141", "Livre III", optionnel)
        
    Returns:
        Structure hi√©rarchique du code ou contenu de la section demand√©e
    """
    client = get_mcp_client()
    return client.call_tool("browse_code", {
        "code_name": code_name,
        "section": section
    })


# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

def get_available_tools() -> List:
    """
    Retourne la liste des outils disponibles pour l'agent.
    
    Returns:
        Liste des fonctions-outils d√©cor√©es avec @tool
    """
    return [search_legifrance, get_article, browse_code]


def test_mcp_connection() -> bool:
    """
    Teste la connexion au serveur MCP.
    
    Returns:
        True si la connexion fonctionne, False sinon
    """
    client = get_mcp_client()
    return client.test_connection()


def get_mcp_server_info() -> Optional[Dict[str, Any]]:
    """
    R√©cup√®re les informations du serveur MCP.
    
    Returns:
        Informations du serveur ou None en cas d'erreur
    """
    client = get_mcp_client()
    return client.get_server_info()
</file>

<file path="streamlit_app.py">
"""
Application Streamlit pour RegulAI - Assistant Juridique IA.

Cette application fournit une interface web pour interagir avec l'agent RegulAI,
sp√©cialis√© dans la recherche juridique fran√ßaise.
"""

import streamlit as st
import os
import uuid
import traceback
from typing import Optional, Dict, Any, Generator

# Configuration de la page
st.set_page_config(
    page_title="RegulAI - Assistant Juridique",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================
# FONCTIONS UTILITAIRES
# ============================================

def generate_thread_id() -> str:
    """G√©n√®re un identifiant unique pour la conversation."""
    return f"streamlit-session-{uuid.uuid4().hex[:8]}"


def get_api_key_from_sources() -> tuple[Optional[str], str]:
    """
    R√©cup√®re la cl√© API OpenAI depuis diff√©rentes sources.
    
    Returns:
        tuple: (cl√©_api, source) o√π source indique d'o√π vient la cl√©
    """
    # V√©rifier les secrets Streamlit
    try:
        api_key_from_secrets = st.secrets.get("OPENAI_API_KEY")
        if api_key_from_secrets:
            return api_key_from_secrets, "secrets.toml"
    except (KeyError, FileNotFoundError):
        pass
    
    # V√©rifier les variables d'environnement
    api_key_from_env = os.getenv("OPENAI_API_KEY")
    if api_key_from_env:
        return api_key_from_env, "variables d'environnement"
    
    return None, "non trouv√©e"


def mask_api_key(api_key: str) -> str:
    """Masque la cl√© API pour l'affichage s√©curis√©."""
    if len(api_key) > 12:
        return f"{api_key[:8]}...{api_key[-4:]}"
    return "cl√© trop courte"


# ============================================
# GESTION DES ERREURS ET VALIDATION
# ============================================

def test_mcp_server_connection() -> tuple[bool, str]:
    """
    Test la connexion au serveur MCP.
    
    Returns:
        tuple: (succ√®s, message_status)
    """
    try:
        from src.regulai.tools import test_mcp_connection
        is_connected = test_mcp_connection()
        if is_connected:
            return True, "‚úÖ Serveur MCP accessible"
        else:
            return False, "‚ùå Serveur MCP non accessible"
    except ImportError:
        return False, "‚ùå Modules RegulAI non disponibles"
    except Exception as e:
        return False, f"‚ùå Erreur de connexion MCP : {str(e)}"


def validate_agent_configuration() -> tuple[bool, str]:
    """
    Valide la configuration de l'agent.
    
    Returns:
        tuple: (valide, message_status)
    """
    try:
        from src.regulai.config import get_config
        config = get_config()
        return True, "‚úÖ Configuration valid√©e"
    except Exception as e:
        return False, f"‚ùå Erreur de configuration : {str(e)}"


# ============================================
# INITIALISATION ET CACHE DE L'AGENT
# ============================================

@st.cache_resource
def initialize_agent(openai_api_key: str):
    """
    Initialise l'agent RegulAI une seule fois et le met en cache.
    
    Args:
        openai_api_key: Cl√© API OpenAI pour l'initialisation
        
    Returns:
        Agent LangGraph compil√© ou None en cas d'erreur
    """
    try:
        # Configurer temporairement la cl√© API dans l'environnement
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # Import et cr√©ation de l'agent
        from src.regulai.agent import create_agent
        agent = create_agent()
        
        return agent
    except ImportError as e:
        st.error(f"‚ùå Erreur d'import des modules RegulAI : {e}")
        return None
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'initialisation de l'agent : {e}")
        return None


# ============================================
# TRAITEMENT DU STREAMING
# ============================================

def process_streaming_events(stream_generator) -> Generator[str, None, None]:
    """
    Traite les √©v√©nements de streaming de l'agent pour afficher les √©tapes interm√©diaires.
    
    Args:
        stream_generator: G√©n√©rateur d'√©v√©nements de streaming de l'agent (mode "updates")
        
    Yields:
        str: Contenu √† afficher pour le streaming de texte
        
    Cette fonction intercepte les √©v√©nements de streaming pour:
    - D√©tecter les appels d'outils et afficher un indicateur de statut
    - Permettre l'affichage des √©tapes interm√©diaires de l'agent
    - Maintenir la compatibilit√© avec st.write_stream
    """
    active_statuses = {}  # Pour traquer les statuses actifs par outil
    last_content = ""  # Pour √©viter la duplication de contenu
    
    try:
        for event in stream_generator:
            # Les √©v√©nements en mode "updates" sont des dictionnaires avec des cl√©s de n≈ìuds
            if isinstance(event, dict):
                # Parcourir chaque n≈ìud dans l'√©v√©nement
                for node_name, node_data in event.items():
                    # V√©rifier s'il y a des messages dans les donn√©es du n≈ìud
                    if isinstance(node_data, dict) and "messages" in node_data:
                        messages = node_data["messages"]
                        if messages:
                            last_message = messages[-1]
                            
                            # D√©tecter les appels d'outils dans les messages AI (n≈ìud "agent")
                            if node_name == "agent" and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                                for tool_call in last_message.tool_calls:
                                    tool_name = tool_call.get('name', 'outil_inconnu')
                                    tool_id = tool_call.get('id', f'tool_{len(active_statuses)}')
                                    
                                    # Mapper les noms d'outils vers des messages plus conviviaux
                                    tool_display_names = {
                                        'search_legifrance': 'üîç Recherche sur L√©gifrance...',
                                        'get_article': 'üìÑ R√©cup√©ration d\'article juridique...',
                                        'browse_code': 'üìö Navigation dans le code juridique...',
                                    }
                                    
                                    status_message = tool_display_names.get(tool_name, f'‚öôÔ∏è Ex√©cution de {tool_name}...')
                                    
                                    # Cr√©er un indicateur de statut pour cet outil
                                    if tool_id not in active_statuses:
                                        active_statuses[tool_id] = st.status(status_message, expanded=False)
                            
                            # D√©tecter les r√©ponses d'outils (n≈ìud "tools")
                            elif node_name == "tools" and hasattr(last_message, 'tool_call_id'):
                                tool_call_id = last_message.tool_call_id
                                # Fermer le statut correspondant s'il existe
                                if tool_call_id in active_statuses:
                                    status = active_statuses[tool_call_id]
                                    status.update(label="‚úÖ Termin√©", state="complete")
                                    # Retirer de la liste des statuses actifs
                                    del active_statuses[tool_call_id]
                            
                            # Si c'est un message de r√©ponse finale de l'agent (sans appels d'outils)
                            elif (node_name == "agent" and 
                                  hasattr(last_message, 'content') and 
                                  last_message.content and
                                  not (hasattr(last_message, 'tool_calls') and last_message.tool_calls)):
                                
                                # Yielder seulement le nouveau contenu pour √©viter la duplication
                                current_content = last_message.content
                                if current_content != last_content:
                                    # Yield le contenu complet (st.write_stream g√®re l'affichage progressif)
                                    yield current_content
                                    last_content = current_content
            else:
                # Si ce n'est pas un √©v√©nement structur√© attendu, essayer de le traiter comme du texte
                content_str = str(event) if event else ""
                if content_str and content_str.strip() and content_str != last_content:
                    yield content_str
                    last_content = content_str
    
    except Exception as e:
        yield f"‚ùå Erreur lors du traitement du streaming : {str(e)}"


# ============================================
# GESTION DE LA CONVERSATION
# ============================================

def handle_user_message(prompt: str, agent) -> Optional[str]:
    """
    Traite un message utilisateur et retourne la r√©ponse de l'agent.
    
    Args:
        prompt: Message de l'utilisateur
        agent: Instance de l'agent RegulAI
        
    Returns:
        R√©ponse de l'agent ou None en cas d'erreur
    """
    try:
        # Import de la fonction de streaming
        from src.regulai.agent import stream_agent_conversation
        
        # Cr√©er le g√©n√©rateur de streaming pour l'agent RegulAI avec d√©tection d'outils
        raw_stream_generator = stream_agent_conversation(
            message=prompt,
            thread_id=st.session_state.thread_id,
            agent=agent
        )
        
        # Traiter les √©v√©nements pour afficher les statuts d'outils
        processed_stream = process_streaming_events(raw_stream_generator)
        
        # Utiliser st.write_stream pour afficher la r√©ponse en temps r√©el
        # st.write_stream consomme le g√©n√©rateur et retourne le contenu complet
        response_content = st.write_stream(processed_stream)
        
        # S'assurer que le retour est toujours une cha√Æne
        if isinstance(response_content, list):
            return "\n".join(str(item) for item in response_content)
        elif response_content is not None:
            return str(response_content)
        else:
            return None
        
    except ImportError as e:
        st.error(f"‚ùå Erreur d'import : {e}")
        return None
    except Exception as e:
        st.error(f"‚ùå Erreur lors du traitement de votre demande : {str(e)}")
        st.error("üí° V√©rifiez votre configuration et r√©essayez.")
        return None


def reset_conversation():
    """R√©initialise compl√®tement la conversation."""
    if "messages" in st.session_state:
        st.session_state.messages = []
    
    # G√©n√©rer un nouveau thread_id pour une nouvelle conversation
    st.session_state.thread_id = generate_thread_id()
    
    # Remettre le message de bienvenue
    welcome_message = {
        "role": "assistant", 
        "content": "Bonjour ! Je suis RegulAI, votre assistant juridique sp√©cialis√© dans le droit fran√ßais. Comment puis-je vous aider aujourd'hui ?"
    }
    st.session_state.messages.append(welcome_message)


# ============================================
# COMPOSANTS DE L'INTERFACE
# ============================================

def render_welcome_section():
    """Affiche la section de bienvenue."""
    st.title("‚öñÔ∏è RegulAI - Assistant Juridique")
    st.subheader("ü§ñ Votre assistant IA sp√©cialis√© en recherche juridique fran√ßaise")
    
    st.markdown("""
    ---
    ### Bienvenue sur RegulAI !

    RegulAI est un assistant IA intelligent sp√©cialis√© dans la recherche juridique fran√ßaise. 
    Il utilise l'architecture LangGraph et des outils MCP pour vous fournir des r√©ponses pr√©cises 
    et actualis√©es sur le droit fran√ßais.

    **Fonctionnalit√©s principales :**
    - üîç Recherche juridique avanc√©e
    - üìö Acc√®s aux bases de donn√©es l√©gales
    - üéØ R√©ponses contextuelles et pr√©cises
    - üí¨ Interface conversationnelle intuitive

    ---
    """)


def render_api_key_configuration():
    """
    G√®re la configuration de la cl√© API OpenAI dans la sidebar.
    
    Returns:
        bool: True si la cl√© API est configur√©e, False sinon
    """
    st.subheader("üîë Cl√© API OpenAI")
    
    # V√©rifier les sources existantes
    api_key_from_sources, source = get_api_key_from_sources()
    
    # Initialiser la session state pour la cl√© API
    if "openai_api_key" not in st.session_state:
        st.session_state.openai_api_key = api_key_from_sources or ""
    
    # Champ de saisie de la cl√© API (type password pour la s√©curit√©)
    user_api_key = st.text_input(
        label="Entrez votre cl√© API OpenAI :",
        type="password",
        value=st.session_state.openai_api_key if source == "non trouv√©e" else "",
        placeholder="sk-..." if source == "non trouv√©e" else f"Cl√© configur√©e via {source}",
        help="Votre cl√© API sera stock√©e de mani√®re s√©curis√©e dans la session.",
        disabled=bool(source != "non trouv√©e"),
        key="api_key_input"
    )
    
    # Mettre √† jour la session state si l'utilisateur a saisi une cl√©
    if user_api_key and source == "non trouv√©e":
        st.session_state.openai_api_key = user_api_key
    
    # Affichage du statut de la cl√© API
    if st.session_state.openai_api_key:
        if source != "non trouv√©e":
            st.success(f"‚úÖ Cl√© API charg√©e depuis {source}")
        else:
            st.success("‚úÖ Cl√© API saisie par l'utilisateur")
        
        # Masquer la cl√© (afficher seulement les premiers et derniers caract√®res)
        masked_key = mask_api_key(st.session_state.openai_api_key)
        st.caption(f"üîí Cl√© active : `{masked_key}`")
        
        return True
    else:
        st.error("‚ùå Aucune cl√© API configur√©e")
        return False


def render_system_status():
    """Affiche l'√©tat du syst√®me dans la sidebar."""
    st.header("üìã √âtat du syst√®me")
    
    # V√©rifier la configuration
    config_valid, config_msg = validate_agent_configuration()
    
    # V√©rifier la connexion MCP
    mcp_connected, mcp_msg = test_mcp_server_connection()
    
    # √âtat de l'agent
    agent_status = "‚ùå Non initialis√©"
    if st.session_state.get("openai_api_key"):
        try:
            agent = initialize_agent(st.session_state.openai_api_key)
            agent_status = "‚úÖ Op√©rationnel" if agent else "‚ùå Erreur d'initialisation"
        except:
            agent_status = "‚ùå Erreur d'initialisation"
    
    st.markdown(f"""
    **Version :** 0.2.0
    
    **√âtat des composants :**
    - üîß Configuration : {config_msg}
    - ü§ñ Agent RegulAI : {agent_status}
    - üîó Serveur MCP : {mcp_msg}
    - ‚úÖ Interface Streamlit : Op√©rationnelle
    
    **Besoin d'aide ?**
    - üìñ Documentation dans le README
    - üîß Exemples dans `/examples`
    - üîë [Obtenir une cl√© API OpenAI](https://platform.openai.com/api-keys)
    """)


def render_conversation_actions():
    """Affiche les actions de conversation dans la sidebar."""
    if not st.session_state.get("openai_api_key"):
        return
    
    st.subheader("üí¨ Actions")
    
    col1, col2 = st.columns(2)
    
    # Bouton pour nouvelle conversation
    with col1:
        if st.button("üÜï Nouvelle", help="D√©marrer une nouvelle conversation", type="primary"):
            reset_conversation()
            st.rerun()
    
    # Bouton pour r√©initialiser l'agent
    with col2:
        if st.button("üîÑ Reset Agent", help="R√©initialiser l'agent en cas de probl√®me", type="secondary"):
            # Vider le cache de l'agent pour le forcer √† se r√©initialiser
            initialize_agent.clear()
            st.rerun()
    
    # Afficher les informations de session
    if "messages" in st.session_state:
        msg_count = len(st.session_state.messages)
        st.caption(f"üìù Messages : {msg_count}")
    
    if "thread_id" in st.session_state:
        st.caption(f"üîó Session : `{st.session_state.thread_id}`")
    
    # Indicateur de streaming
    st.caption("üí¨ Mode streaming activ√©")


def render_chat_interface():
    """Affiche l'interface de chat principale."""
    # Initialiser l'identifiant de conversation unique
    if "thread_id" not in st.session_state:
        st.session_state.thread_id = generate_thread_id()
    
    # Initialiser l'historique de conversation dans session_state
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # Message de bienvenue
        welcome_message = {
            "role": "assistant", 
            "content": "Bonjour ! Je suis RegulAI, votre assistant juridique sp√©cialis√© dans le droit fran√ßais. Comment puis-je vous aider aujourd'hui ?"
        }
        st.session_state.messages.append(welcome_message)
    
    # Afficher l'historique des messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Champ de saisie pour les nouveaux messages
    if prompt := st.chat_input("Posez votre question juridique..."):
        # Ajouter le message utilisateur √† l'historique
        user_message = {"role": "user", "content": prompt}
        st.session_state.messages.append(user_message)
        
        # Afficher le message utilisateur
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Obtenir l'agent initialis√©
        try:
            agent = initialize_agent(st.session_state.openai_api_key)
            
            if agent:
                # Afficher la r√©ponse de l'agent avec streaming et statut des outils
                with st.chat_message("assistant"):
                    response_content = handle_user_message(prompt, agent)
                    
                    if response_content:
                        # Ajouter la r√©ponse √† l'historique
                        assistant_message = {"role": "assistant", "content": response_content}
                        st.session_state.messages.append(assistant_message)
                    else:
                        # En cas d'erreur, ajouter un message d'erreur g√©n√©rique
                        error_msg = "‚ùå Impossible de traiter votre demande. Veuillez r√©essayer."
                        st.markdown(error_msg)
                        assistant_message = {"role": "assistant", "content": error_msg}
                        st.session_state.messages.append(assistant_message)
            else:
                # Agent non initialis√© - afficher un message d'erreur
                error_msg = "‚ùå **Agent non disponible**\n\nL'agent RegulAI n'a pas pu √™tre initialis√©. V√©rifiez votre cl√© API OpenAI."
                
                with st.chat_message("assistant"):
                    st.markdown(error_msg)
                
                assistant_message = {"role": "assistant", "content": error_msg}
                st.session_state.messages.append(assistant_message)
                
        except Exception as e:
            # Erreur g√©n√©rale d'initialisation
            error_msg = f"‚ùå **Erreur syst√®me**\n\n{str(e)}"
            
            with st.chat_message("assistant"):
                st.markdown(error_msg)
            
            assistant_message = {"role": "assistant", "content": error_msg}
            st.session_state.messages.append(assistant_message)


def render_configuration_warning():
    """Affiche les instructions de configuration si n√©cessaire."""
    st.warning("""
    ‚ö†Ô∏è **Configuration requise**
    
    Pour utiliser RegulAI, vous devez configurer votre cl√© API OpenAI. 
    
    **Options de configuration :**
    
    1. **Via la barre lat√©rale** ‚Üê Saisissez votre cl√© dans le champ √† gauche
    2. **Via un fichier secrets.toml** ‚Üê Cr√©ez `.streamlit/secrets.toml` :
       ```toml
       OPENAI_API_KEY = "sk-votre-cl√©-ici"
       ```
    3. **Via les variables d'environnement** ‚Üê D√©finissez `OPENAI_API_KEY`
    
    [üîó Obtenir une cl√© API OpenAI](https://platform.openai.com/api-keys)
    """)
    
    st.info("üí° **Votre cl√© API est s√©curis√©e** - Elle est stock√©e uniquement dans votre session et jamais transmise ailleurs que vers l'API OpenAI.")


# ============================================
# APPLICATION PRINCIPALE
# ============================================

def main():
    """Fonction principale de l'application."""
    try:
        # Afficher la section de bienvenue
        render_welcome_section()
        
        # Sidebar - Configuration et √©tat
        with st.sidebar:
            st.header("üîß Configuration")
            
            # Configuration de la cl√© API
            api_key_configured = render_api_key_configuration()
            
            st.divider()
            
            # Actions de conversation
            render_conversation_actions()
            
            st.divider()
            
            # √âtat du syst√®me
            render_system_status()
        
        # Interface principale
        if not api_key_configured:
            render_configuration_warning()
        else:
            st.success("‚úÖ Configuration termin√©e ! Vous pouvez maintenant commencer √† converser avec RegulAI.")
            render_chat_interface()
    
    except Exception as e:
        st.error(f"‚ùå **Erreur critique de l'application**")
        st.error(f"D√©tails : {str(e)}")
        
        # Afficher la stack trace en mode d√©veloppement
        if st.checkbox("üîß Afficher les d√©tails techniques", help="Pour le debugging"):
            st.code(traceback.format_exc())
        
        st.info("üí° Essayez de recharger la page ou contactez le support si le probl√®me persiste.")


if __name__ == "__main__":
    main()
</file>

<file path="test_app_final.py">
#!/usr/bin/env python3
"""
Script de test pour valider l'application Streamlit refactoris√©e.

Ce script teste les diff√©rentes fonctions modulaires de l'application
sans avoir besoin de lancer Streamlit complet.
"""

import os
import sys
import tempfile
import traceback
from unittest.mock import Mock, patch

# Ajouter le r√©pertoire racine au chemin Python
sys.path.insert(0, os.path.abspath('.'))

def test_utility_functions():
    """Test des fonctions utilitaires."""
    print("üß™ Test des fonctions utilitaires...")
    
    try:
        # Import des fonctions depuis streamlit_app
        from streamlit_app import generate_thread_id, mask_api_key
        
        # Test de g√©n√©ration de thread_id
        thread_id = generate_thread_id()
        assert thread_id.startswith("streamlit-session-"), f"Thread ID invalide: {thread_id}"
        assert len(thread_id) > 20, "Thread ID trop court"
        print(f"  ‚úÖ generate_thread_id: {thread_id}")
        
        # Test de masquage de cl√© API
        test_key = "sk-1234567890abcdefghijklmnopqrstuvwxyz1234"
        masked = mask_api_key(test_key)
        expected = "sk-12345...1234"
        assert masked == expected, f"Masquage incorrect: {masked} != {expected}"
        print(f"  ‚úÖ mask_api_key: {masked}")
        
        # Test avec cl√© courte
        short_key = "sk-123"
        masked_short = mask_api_key(short_key)
        assert masked_short == "cl√© trop courte", f"Gestion cl√© courte incorrecte: {masked_short}"
        print("  ‚úÖ mask_api_key (cl√© courte): cl√© trop courte")
        
        print("‚úÖ Fonctions utilitaires: OK\n")
        
    except Exception as e:
        print(f"‚ùå Erreur dans test_utility_functions: {e}")
        traceback.print_exc()
        return False
    
    return True


def test_api_key_sources():
    """Test de r√©cup√©ration des cl√©s API depuis diff√©rentes sources."""
    print("üß™ Test de r√©cup√©ration des cl√©s API...")
    
    try:
        # Mock Streamlit
        mock_st = Mock()
        mock_st.secrets = {"OPENAI_API_KEY": "sk-from-secrets"}
        
        with patch('streamlit_app.st', mock_st):
            from streamlit_app import get_api_key_from_sources
            
            # Test avec secrets Streamlit
            api_key, source = get_api_key_from_sources()
            assert api_key == "sk-from-secrets", f"Cl√© from secrets incorrecte: {api_key}"
            assert source == "secrets.toml", f"Source incorrecte: {source}"
            print(f"  ‚úÖ Secrets Streamlit: {api_key} depuis {source}")
        
        # Test avec variable d'environnement
        test_env_key = "sk-from-env-test"
        with patch.dict(os.environ, {"OPENAI_API_KEY": test_env_key}):
            # Mock st.secrets pour qu'il l√®ve une KeyError
            mock_st_no_secrets = Mock()
            mock_st_no_secrets.secrets.get.side_effect = KeyError("No secrets")
            
            with patch('streamlit_app.st', mock_st_no_secrets):
                api_key, source = get_api_key_from_sources()
                assert api_key == test_env_key, f"Cl√© from env incorrecte: {api_key}"
                assert source == "variables d'environnement", f"Source incorrecte: {source}"
                print(f"  ‚úÖ Variables d'environnement: {api_key} depuis {source}")
        
        # Test sans cl√©
        mock_st_empty = Mock()
        mock_st_empty.secrets.get.side_effect = KeyError("No secrets")
        
        with patch('streamlit_app.st', mock_st_empty), \
             patch.dict(os.environ, {}, clear=True):
            # S'assurer qu'OPENAI_API_KEY n'est pas dans l'environnement
            if 'OPENAI_API_KEY' in os.environ:
                del os.environ['OPENAI_API_KEY']
            
            api_key, source = get_api_key_from_sources()
            assert api_key is None, f"Cl√© devrait √™tre None: {api_key}"
            assert source == "non trouv√©e", f"Source incorrecte: {source}"
            print(f"  ‚úÖ Aucune cl√©: {api_key} - {source}")
        
        print("‚úÖ R√©cup√©ration cl√©s API: OK\n")
        
    except Exception as e:
        print(f"‚ùå Erreur dans test_api_key_sources: {e}")
        traceback.print_exc()
        return False
    
    return True


def test_validation_functions():
    """Test des fonctions de validation."""
    print("üß™ Test des fonctions de validation...")
    
    try:
        # Mock Streamlit pour √©viter les erreurs d'import
        mock_st = Mock()
        
        with patch('streamlit_app.st', mock_st):
            from streamlit_app import validate_agent_configuration, test_mcp_server_connection
            
            # Test de validation de configuration
            try:
                valid, msg = validate_agent_configuration()
                print(f"  ‚ÑπÔ∏è  Configuration agent: {msg}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Configuration agent: Erreur attendue - {e}")
            
            # Test de connexion MCP
            try:
                connected, msg = test_mcp_server_connection()
                print(f"  ‚ÑπÔ∏è  Connexion MCP: {msg}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Connexion MCP: Erreur attendue - {e}")
        
        print("‚úÖ Fonctions de validation: OK\n")
        
    except Exception as e:
        print(f"‚ùå Erreur dans test_validation_functions: {e}")
        traceback.print_exc()
        return False
    
    return True


def test_error_handling():
    """Test de la gestion d'erreurs."""
    print("üß™ Test de la gestion d'erreurs...")
    
    try:
        # Mock Streamlit
        mock_st = Mock()
        
        with patch('streamlit_app.st', mock_st):
            from streamlit_app import process_streaming_events
            
            # Test avec g√©n√©rateur qui l√®ve une exception
            def failing_generator():
                yield {"agent": {"messages": [Mock(content="test")]}}
                raise ValueError("Erreur de test")
            
            # Collecter les r√©sultats du g√©n√©rateur
            results = list(process_streaming_events(failing_generator()))
            
            # V√©rifier qu'il y a au moins un message d'erreur
            error_messages = [r for r in results if "‚ùå Erreur lors du traitement du streaming" in r]
            assert len(error_messages) > 0, "Aucun message d'erreur g√©n√©r√©"
            print(f"  ‚úÖ Gestion d'erreur streaming: {error_messages[0][:50]}...")
        
        print("‚úÖ Gestion d'erreurs: OK\n")
        
    except Exception as e:
        print(f"‚ùå Erreur dans test_error_handling: {e}")
        traceback.print_exc()
        return False
    
    return True


def test_import_structure():
    """Test de la structure d'import de l'application."""
    print("üß™ Test de la structure d'import...")
    
    try:
        # Tester l'import de l'application principale
        import streamlit_app
        
        # V√©rifier que les fonctions principales existent
        required_functions = [
            'main',
            'generate_thread_id',
            'get_api_key_from_sources',
            'mask_api_key',
            'test_mcp_server_connection',
            'validate_agent_configuration',
            'initialize_agent',
            'process_streaming_events',
            'handle_user_message',
            'reset_conversation',
            'render_welcome_section',
            'render_api_key_configuration',
            'render_system_status',
            'render_conversation_actions',
            'render_chat_interface',
            'render_configuration_warning'
        ]
        
        for func_name in required_functions:
            assert hasattr(streamlit_app, func_name), f"Fonction manquante: {func_name}"
            print(f"  ‚úÖ {func_name}")
        
        print("‚úÖ Structure d'import: OK\n")
        
    except Exception as e:
        print(f"‚ùå Erreur dans test_import_structure: {e}")
        traceback.print_exc()
        return False
    
    return True


def test_mock_conversation():
    """Test simul√© d'une conversation."""
    print("üß™ Test simul√© d'une conversation...")
    
    try:
        from streamlit_app import generate_thread_id
        
        # Test de g√©n√©ration de thread_id
        thread_id = generate_thread_id()
        assert thread_id.startswith("streamlit-session-"), "Thread ID invalide"
        print(f"  ‚úÖ Thread ID g√©n√©r√©: {thread_id}")
        
        # Test d'import des fonctions principales
        from streamlit_app import handle_user_message, reset_conversation
        
        # V√©rifier que les fonctions existent
        assert callable(handle_user_message), "handle_user_message n'est pas callable"
        assert callable(reset_conversation), "reset_conversation n'est pas callable"
        print("  ‚úÖ Fonctions de conversation import√©es")
        
        # Test que la fonction existe et peut √™tre appel√©e (test basique)
        print("  ‚úÖ Fonction handle_user_message test√©e")
        
        # Test des types de retour attendus
        assert hasattr(generate_thread_id, '__call__'), "generate_thread_id doit √™tre callable"
        print("  ‚úÖ Tests de types: OK")
        
        print("‚úÖ Test conversation simul√©e: OK\n")
        
    except Exception as e:
        print(f"‚ùå Erreur dans test_mock_conversation: {e}")
        traceback.print_exc()
        return False
    
    return True


def main():
    """Fonction principale des tests."""
    print("üöÄ D√©but des tests de l'application Streamlit refactoris√©e\n")
    
    tests = [
        test_import_structure,
        test_utility_functions,
        test_api_key_sources,
        test_validation_functions,
        test_error_handling,
        test_mock_conversation
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"‚ùå Test {test.__name__} a √©chou√© avec l'exception: {e}")
            failed += 1
    
    print("=" * 60)
    print(f"üìä R√©sultats des tests:")
    print(f"   ‚úÖ Tests r√©ussis: {passed}")
    print(f"   ‚ùå Tests √©chou√©s: {failed}")
    print(f"   üìà Taux de r√©ussite: {(passed/(passed+failed)*100):.1f}%")
    
    if failed == 0:
        print("\nüéâ Tous les tests sont pass√©s ! L'application est pr√™te.")
        return True
    else:
        print(f"\n‚ö†Ô∏è {failed} test(s) ont √©chou√©. V√©rifiez les erreurs ci-dessus.")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
</file>

<file path="test_streaming_mock.py">
#!/usr/bin/env python3
"""
Script de test avec mock pour v√©rifier le streaming avec d√©tection d'outils.

Ce script simule les √©v√©nements LangGraph pour tester la logique de streaming
sans avoir besoin d'une cl√© API OpenAI valide.
"""

from typing import Dict, Any, Generator, List
from dataclasses import dataclass, field

@dataclass
class MockMessage:
    """Message simul√© pour les tests."""
    content: str = ""
    tool_calls: List[Dict[str, Any]] = field(default_factory=list)
    tool_call_id: str = ""
    
    def __post_init__(self):
        if self.tool_calls is None:
            self.tool_calls = []

def simulate_status_display(message: str, expanded: bool = False):
    """Simule l'affichage d'un st.status."""
    print(f"üìä STATUS: {message} (expanded={expanded})")
    return {"message": message, "expanded": expanded, "state": "running"}

def simulate_status_update(status_obj: Dict[str, Any], label: str, state: str):
    """Simule la mise √† jour d'un st.status."""
    print(f"üîÑ STATUS UPDATE: {label} (state={state})")
    status_obj["message"] = label
    status_obj["state"] = state

def mock_stream_agent_conversation() -> Generator[Dict[str, Any], None, None]:
    """
    G√©n√©rateur qui simule les √©v√©nements de streaming d'un agent LangGraph.
    
    Cette fonction simule la s√©quence typique :
    1. L'agent d√©cide d'utiliser un outil
    2. L'outil est ex√©cut√© 
    3. L'agent g√©n√®re une r√©ponse finale
    """
    
    # √âtape 1: L'agent d√©cide d'utiliser un outil de recherche
    yield {
        "agent": {
            "messages": [
                MockMessage(
                    content="",
                    tool_calls=[{
                        "name": "search_legifrance",
                        "id": "tool_call_123",
                        "args": {"query": "cong√©s pay√©s", "max_results": 10}
                    }]
                )
            ]
        }
    }
    
    # √âtape 2: L'outil retourne un r√©sultat
    yield {
        "tools": {
            "messages": [
                MockMessage(
                    content="R√©sultats de recherche trouv√©s: Articles L3141-1 √† L3141-32 du Code du travail...",
                    tool_call_id="tool_call_123"
                )
            ]
        }
    }
    
    # √âtape 3: L'agent g√©n√®re une r√©ponse finale
    yield {
        "agent": {
            "messages": [
                MockMessage(
                    content="Voici les informations sur les cong√©s pay√©s en France..."
                )
            ]
        }
    }

def test_process_streaming_events():
    """Test de la fonction process_streaming_events avec des donn√©es simul√©es."""
    print("üß™ Test de la logique de streaming avec d√©tection d'outils")
    print("=" * 70)
    
    # Variables pour simuler le comportement de Streamlit
    active_statuses = {}
    last_content = ""
    yielded_content = []
    
    print("üé¨ Simulation d'√©v√©nements de streaming...")
    print("-" * 50)
    
    # Simuler la logique de process_streaming_events
    for event in mock_stream_agent_conversation():
        print(f"üì• √âv√©nement re√ßu: {list(event.keys())}")
        
        if isinstance(event, dict):
            # Parcourir chaque n≈ìud dans l'√©v√©nement
            for node_name, node_data in event.items():
                print(f"   üî∏ N≈ìud: {node_name}")
                
                # V√©rifier s'il y a des messages dans les donn√©es du n≈ìud
                if isinstance(node_data, dict) and "messages" in node_data:
                    messages = node_data["messages"]
                    if messages:
                        last_message = messages[-1]
                        print(f"   üìß Message: {type(last_message).__name__}")
                        
                        # D√©tecter les appels d'outils dans les messages AI (n≈ìud "agent")
                        if node_name == "agent" and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                            print("   üîß D√âTECTION D'APPEL D'OUTIL!")
                            for tool_call in last_message.tool_calls:
                                tool_name = tool_call.get('name', 'outil_inconnu')
                                tool_id = tool_call.get('id', f'tool_{len(active_statuses)}')
                                
                                # Mapper les noms d'outils vers des messages plus conviviaux
                                tool_display_names = {
                                    'search_legifrance': 'üîç Recherche sur L√©gifrance...',
                                    'get_article': 'üìÑ R√©cup√©ration d\'article juridique...',
                                    'browse_code': 'üìö Navigation dans le code juridique...',
                                }
                                
                                status_message = tool_display_names.get(tool_name, f'‚öôÔ∏è Ex√©cution de {tool_name}...')
                                
                                # Cr√©er un indicateur de statut pour cet outil
                                if tool_id not in active_statuses:
                                    active_statuses[tool_id] = simulate_status_display(status_message)
                                    print(f"   üÜï Nouveau statut cr√©√© pour {tool_name} (ID: {tool_id})")
                        
                        # D√©tecter les r√©ponses d'outils (n≈ìud "tools")
                        elif node_name == "tools" and hasattr(last_message, 'tool_call_id'):
                            print("   ‚úÖ D√âTECTION DE R√âPONSE D'OUTIL!")
                            tool_call_id = last_message.tool_call_id
                            # Fermer le statut correspondant s'il existe
                            if tool_call_id in active_statuses:
                                simulate_status_update(
                                    active_statuses[tool_call_id], 
                                    "‚úÖ Termin√©", 
                                    "complete"
                                )
                                # Retirer de la liste des statuses actifs
                                del active_statuses[tool_call_id]
                                print(f"   üèÅ Statut ferm√© pour {tool_call_id}")
                        
                        # Si c'est un message de r√©ponse finale de l'agent (sans appels d'outils)
                        elif (node_name == "agent" and 
                              hasattr(last_message, 'content') and 
                              last_message.content and
                              not (hasattr(last_message, 'tool_calls') and last_message.tool_calls)):
                            
                            print("   üí¨ D√âTECTION DE CONTENU DE R√âPONSE!")
                            # Yielder seulement le nouveau contenu pour √©viter la duplication
                            current_content = last_message.content
                            if current_content != last_content:
                                # Simuler le yield du contenu
                                yielded_content.append(current_content)
                                print(f"   üì§ Contenu yielded: {current_content[:80]}{'...' if len(current_content) > 80 else ''}")
                                last_content = current_content
        
        print()  # Ligne vide pour s√©parer les √©v√©nements
    
    print("=" * 70)
    print("‚úÖ Test termin√© avec succ√®s !")
    print(f"üìä Statuts actifs restants: {len(active_statuses)}")
    print(f"üìù Contenu yielded total: {len(yielded_content)} √©l√©ments")
    
    if yielded_content:
        print("\nüìã R√©sum√© du contenu yielded:")
        for i, content in enumerate(yielded_content, 1):
            print(f"   {i}. {content[:60]}{'...' if len(content) > 60 else ''}")

def test_edge_cases():
    """Test des cas limites et edge cases."""
    print("\nüß™ Test des cas limites")
    print("=" * 40)
    
    # Test avec √©v√©nement vide
    print("üì• Test √©v√©nement vide...")
    event = {}
    print(f"   R√©sultat: {isinstance(event, dict)} (dict vide)")
    
    # Test avec message sans tool_calls
    print("üì• Test message sans tool_calls...")
    message = MockMessage(content="Test simple")
    print(f"   tool_calls: {message.tool_calls}")
    print(f"   hasattr tool_calls: {hasattr(message, 'tool_calls')}")
    
    # Test avec message ayant tool_calls vide
    print("üì• Test message avec tool_calls vide...")
    message_with_empty_calls = MockMessage(content="Test", tool_calls=[])
    print(f"   tool_calls: {message_with_empty_calls.tool_calls}")
    print(f"   bool(tool_calls): {bool(message_with_empty_calls.tool_calls)}")
    
    print("‚úÖ Tests des cas limites termin√©s")

if __name__ == "__main__":
    test_process_streaming_events()
    test_edge_cases()
</file>

<file path="test_streaming.py">
#!/usr/bin/env python3
"""
Script de test pour v√©rifier le streaming avec d√©tection d'outils.

Ce script simule le comportement de l'application Streamlit pour tester
la logique de streaming et d'affichage des statuts d'outils.
"""

import os
from typing import Dict, Any

# Configuration temporaire de la cl√© API pour le test
# Assurez-vous d'avoir une cl√© API valide dans votre .env
from dotenv import load_dotenv
load_dotenv()

def simulate_status_display(message: str, expanded: bool = False):
    """Simule l'affichage d'un st.status."""
    print(f"üìä STATUS: {message} (expanded={expanded})")
    return {"message": message, "expanded": expanded, "state": "running"}

def simulate_status_update(status_obj: Dict[str, Any], label: str, state: str):
    """Simule la mise √† jour d'un st.status."""
    print(f"üîÑ STATUS UPDATE: {label} (state={state})")
    status_obj["message"] = label
    status_obj["state"] = state

def test_streaming_with_tool_detection():
    """Test la fonction de streaming avec d√©tection d'outils."""
    print("üß™ Test du streaming avec d√©tection d'outils")
    print("=" * 60)
    
    try:
        # Import des fonctions n√©cessaires
        from src.regulai.agent import create_agent, stream_agent_conversation
        
        # Cr√©er l'agent
        print("üìù Cr√©ation de l'agent...")
        agent = create_agent()
        print("‚úÖ Agent cr√©√© avec succ√®s")
        
        # Message de test qui devrait d√©clencher un appel d'outil
        test_message = "Peux-tu rechercher des informations sur les cong√©s pay√©s en France ?"
        print(f"\nüë§ Message de test: {test_message}")
        print("\nü§ñ Streaming de la r√©ponse:")
        print("-" * 40)
        
        # Variables pour simuler le comportement de Streamlit
        active_statuses = {}
        last_content = ""
        
        # Streamer la conversation
        for event in stream_agent_conversation(test_message, "test-streaming", agent):
            print(f"üì• √âv√©nement re√ßu: {type(event)}")
            
            # Simuler la logique de process_streaming_events
            if isinstance(event, dict):
                for node_name, node_data in event.items():
                    print(f"   üî∏ N≈ìud: {node_name}")
                    
                    if isinstance(node_data, dict) and "messages" in node_data:
                        messages = node_data["messages"]
                        if messages:
                            last_message = messages[-1]
                            print(f"   üìß Message: {type(last_message).__name__}")
                            
                            # D√©tecter les appels d'outils
                            if (node_name == "agent" and 
                                hasattr(last_message, 'tool_calls') and 
                                last_message.tool_calls):
                                
                                print("   üîß D√âTECTION D'APPEL D'OUTIL!")
                                for tool_call in last_message.tool_calls:
                                    tool_name = tool_call.get('name', 'outil_inconnu')
                                    tool_id = tool_call.get('id', f'tool_{len(active_statuses)}')
                                    
                                    tool_display_names = {
                                        'search_legifrance': 'üîç Recherche sur L√©gifrance...',
                                        'get_article': 'üìÑ R√©cup√©ration d\'article juridique...',
                                        'browse_code': 'üìö Navigation dans le code juridique...',
                                    }
                                    
                                    status_message = tool_display_names.get(tool_name, f'‚öôÔ∏è Ex√©cution de {tool_name}...')
                                    
                                    if tool_id not in active_statuses:
                                        active_statuses[tool_id] = simulate_status_display(status_message)
                                        print(f"   üÜï Nouveau statut cr√©√© pour {tool_name}")
                            
                            # D√©tecter les r√©ponses d'outils
                            elif (node_name == "tools" and 
                                  hasattr(last_message, 'tool_call_id')):
                                
                                print("   ‚úÖ D√âTECTION DE R√âPONSE D'OUTIL!")
                                tool_call_id = last_message.tool_call_id
                                if tool_call_id in active_statuses:
                                    simulate_status_update(
                                        active_statuses[tool_call_id], 
                                        "‚úÖ Termin√©", 
                                        "complete"
                                    )
                                    del active_statuses[tool_call_id]
                                    print(f"   üèÅ Statut ferm√© pour {tool_call_id}")
                            
                            # D√©tecter le contenu de r√©ponse
                            elif (node_name == "agent" and 
                                  hasattr(last_message, 'content') and 
                                  last_message.content and
                                  not (hasattr(last_message, 'tool_calls') and last_message.tool_calls)):
                                
                                current_content = last_message.content
                                if current_content != last_content:
                                    print("   üí¨ CONTENU DE R√âPONSE:")
                                    print(f"      {current_content[:100]}{'...' if len(current_content) > 100 else ''}")
                                    last_content = current_content
        
        print("\n" + "=" * 60)
        print("‚úÖ Test termin√© avec succ√®s !")
        print(f"üìä Statuts actifs restants: {len(active_statuses)}")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors du test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_streaming_with_tool_detection()
</file>

<file path="tests/__init__.py">
# Tests pour le package RegulAI
</file>

<file path="tests/conftest.py">
"""
Configuration pytest pour les tests RegulAI.

Ce fichier contient les fixtures communes et la configuration de test.
"""

import pytest
import os
import sys
from unittest.mock import Mock

# Ajouter le r√©pertoire src au PYTHONPATH
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from regulai.config import RegulAIConfig


@pytest.fixture
def mock_config():
    """Fixture qui fournit une configuration mock√©e pour les tests."""
    config = RegulAIConfig(
        openai_api_key="test_key_123",
        mcp_server_url="http://localhost:8000",
        mcp_timeout=30,
        model_name="gpt-4o-mini",
        model_temperature=0.0,
        max_iterations=20,
        default_max_results=10,
        log_level="INFO"
    )
    return config


@pytest.fixture
def mock_mcp_client():
    """Fixture qui fournit un client MCP mock√©."""
    client = Mock()
    client.call_tool.return_value = "R√©ponse de test du serveur MCP"
    client.test_connection.return_value = True
    client.get_server_info.return_value = {"version": "1.0.0", "status": "running"}
    return client


@pytest.fixture
def mock_openai_response():
    """Fixture qui fournit une r√©ponse OpenAI mock√©e."""
    response = Mock()
    response.content = "R√©ponse de test de l'agent"
    response.tool_calls = []
    return response
</file>

<file path="tests/test_tools.py">
"""
Tests pour les outils RegulAI.

Ce module teste les fonctionnalit√©s des outils MCP et leur int√©gration LangChain.
"""

import pytest
from unittest.mock import patch, Mock, MagicMock
import os
import sys

# Ajouter le r√©pertoire src au PYTHONPATH
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from regulai.tools import get_available_tools, MCPClient, search_legifrance


def test_get_available_tools():
    """Test que les outils sont correctement charg√©s."""
    tools = get_available_tools()
    
    assert len(tools) == 3
    tool_names = [tool.name for tool in tools]
    assert "search_legifrance" in tool_names
    assert "get_article" in tool_names
    assert "browse_code" in tool_names


def test_mcp_client_initialization():
    """Test l'initialisation du client MCP."""
    client = MCPClient("http://test:8000", 60)
    
    assert client.server_url == "http://test:8000"
    assert client.timeout == 60


def test_search_legifrance_tool_structure():
    """Test la structure de l'outil search_legifrance."""
    tools = get_available_tools()
    search_tool = next(tool for tool in tools if tool.name == "search_legifrance")
    
    assert search_tool.name == "search_legifrance"
    assert search_tool.description is not None
    assert "Recherche des textes juridiques" in search_tool.description
    
    # V√©rifier le sch√©ma d'arguments
    schema = search_tool.args_schema.model_json_schema()
    properties = schema.get("properties", {})
    assert "query" in properties
    assert "max_results" in properties


@patch('regulai.tools.get_mcp_client')
def test_search_legifrance_tool_call(mock_get_client):
    """Test l'appel de l'outil search_legifrance."""
    # Mock du client MCP
    mock_client = Mock()
    mock_client.call_tool.return_value = "R√©sultats de recherche mock√©s"
    mock_get_client.return_value = mock_client
    
    # Appeler l'outil directement avec invoke
    result = search_legifrance.invoke({"query": "test query", "max_results": 5})
    
    # V√©rifications
    assert result == "R√©sultats de recherche mock√©s"
    mock_client.call_tool.assert_called_once_with(
        "search_legifrance", 
        {"query": "test query", "max_results": 5}
    )


def test_mcp_client_parse_response():
    """Test le parsing des r√©ponses MCP."""
    client = MCPClient()
    
    # Test r√©ponse normale
    response = {
        "result": {
            "content": [
                {"text": "Contenu de test"}
            ]
        }
    }
    
    result = client._parse_mcp_response(response)
    assert result == "Contenu de test"
    
    # Test r√©ponse simple
    response_simple = {
        "result": "R√©ponse simple"
    }
    
    result_simple = client._parse_mcp_response(response_simple)
    assert result_simple == "R√©ponse simple"


@patch('httpx.Client')
def test_mcp_client_connection_test(mock_httpx_client):
    """Test la v√©rification de connexion MCP."""
    
    # Mock de la r√©ponse HTTP
    mock_response = Mock()
    mock_response.status_code = 200
    
    # Cr√©er un mock pour la session HTTP qui fonctionne comme context manager
    mock_session = Mock()
    mock_session.get.return_value = mock_response
    
    # Utiliser MagicMock pour supporter automatiquement les context managers
    mock_context_manager = MagicMock()
    mock_context_manager.__enter__.return_value = mock_session
    mock_context_manager.__exit__.return_value = None
    
    # Le Client() retourne le context manager
    mock_httpx_client.return_value = mock_context_manager
    
    # Test de connexion
    client = MCPClient("http://test:8000")
    result = client.test_connection()
    
    assert result is True
    mock_session.get.assert_called_once_with("http://test:8000/health")
</file>

</files>
